Tuyệt vời. Mình sẽ viết một bài chuyên sâu trình bày về Stochastic Approximation trong Reinforcement Learning, dựa trên các nguồn chính như *Reinforcement Learning: An Introduction* của Sutton và Barto, *Mathematical Foundations of Reinforcement Learning* của Shiyu Zhao, cùng với các tài liệu học thuật liên quan.

Mình sẽ trình bày bài viết theo cấu trúc rõ ràng, giải thích chi tiết về khái niệm, lịch sử phát triển, các thuật toán chính, mối liên hệ với RL, cũng như các kết quả hội tụ quan trọng và ứng dụng thực tế. Mình sẽ cập nhật cho bạn khi hoàn thành bản nháp đầu tiên.

# Xấp xỉ ngẫu nhiên trong học tăng cường (Reinforcement Learning)

## Giới thiệu tổng quan về Xấp xỉ Ngẫu nhiên (SA)
**Xấp xỉ ngẫu nhiên** (tiếng Anh: *Stochastic Approximation* - viết tắt SA) là một họ các phương pháp lặp được phát triển nhằm giải các bài toán tìm nghiệm (root-finding) hoặc tối ưu hàm trong bối cảnh có nhiễu ngẫu nhiên ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Stochastic%20approximation%20methods%20are%20a,only%20estimated%20via%20noisy%20observations)). Thay vì tính toán chính xác trên toàn bộ dữ liệu (vốn có thể tốn kém hoặc không khả thi), thuật toán SA cập nhật xấp xỉ nghiệm bằng cách sử dụng **mẫu quan sát ngẫu nhiên** – ví dụ như giá trị hàm bị nhiễu hoặc gradient ước lượng – rồi hiệu chỉnh dần dần qua nhiều bước. Nói cách khác, SA giúp ta **tiệm cận** đến lời giải của một phương trình hoặc cực trị hàm mục tiêu thông qua các quan sát ngẫu nhiên, mà không cần biết tường minh dạng hàm kỳ vọng cơ bản ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=In%20a%20nutshell%2C%20stochastic%20approximation,such%20as%20zeros%20or%20extrema)).

Lý do hình thành phương pháp SA xuất phát từ nhu cầu giải quyết các bài toán ước lượng và tối ưu trong thực nghiệm khi dữ liệu thu thập luôn nhiễu. Vào đầu thập niên 1950, các nhà toán học đã đề xuất những thuật toán SA tiên phong: **Robbins–Monro (1951)** và **Kiefer–Wolfowitz (1952)**. Đây được xem là hai thuật toán nguyên mẫu đầu tiên đặt nền móng cho lĩnh vực SA ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=The%20earliest%2C%20and%20prototypical%2C%20algorithms,respectively%20in%201951%20and%201952)). Thuật toán Robbins–Monro tập trung vào bài toán tìm *nghiệm gốc của một hàm kỳ vọng*, còn thuật toán Kiefer–Wolfowitz nhằm *tối ưu giá trị cực đại của hàm mục tiêu* khi chỉ quan sát được các đánh giá nhiễu của hàm. Chi tiết về hai thuật toán kinh điển này sẽ được trình bày ở phần sau.

Sau Robbins–Monro và Kiefer–Wolfowitz, lĩnh vực SA tiếp tục phát triển mạnh mẽ. Nhiều kết quả lý thuyết quan trọng đã được thiết lập (ví dụ: Blum (1954) chứng minh hội tụ gần như chắc chắn thay vì chỉ hội tụ xác suất ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Kiefer%20and%20Wolfowitz%20proved%20that%2C,almost%20surely%2C%20provided%20that))). Về ứng dụng, SA ngày càng phổ biến trong thống kê và **machine learning**, đặc biệt khi xử lý dữ liệu lớn và học trực tuyến. Các thuật toán tối ưu ngẫu nhiên (như **stochastic gradient descent**), các biến thể trực tuyến của thuật toán EM, và đặc biệt là **học tăng cường (reinforcement learning) sử dụng phương pháp sai phân thời gian (temporal differences)** đều có thể được xem là các trường hợp áp dụng của nguyên lý SA ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Recently%2C%20stochastic%20approximations%20have%20found,2)). Nhờ khả năng xử lý nhiễu và học dần dần, SA đã trở thành một công cụ cốt lõi cho nhiều thuật toán học máy hiện đại, bao gồm cả **học sâu** trong những năm gần đây ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Recently%2C%20stochastic%20approximations%20have%20found,2)).

## Vai trò của SA trong Học tăng cường
**Học tăng cường (Reinforcement Learning - RL)** thường xuyên đối mặt với môi trường không chắc chắn và phản hồi mang tính ngẫu nhiên (phần thưởng, chuyển trạng thái). Do đó, hầu hết các thuật toán RL đều học từ **mẫu quan sát** (episode, bước trạng thái–hành động) theo kiểu từng bước một, thay vì tính toán chính xác giá trị kỳ vọng lý thuyết. Chính điều này làm cho phương pháp xấp xỉ ngẫu nhiên trở thành nền tảng tự nhiên cho RL: mỗi lần agent tương tác với môi trường, nó cập nhật một lượng nhỏ vào các ước lượng giá trị hoặc chính sách của mình, tương tự như cách SA cập nhật nghiệm bằng các quan sát ngẫu nhiên.

Thực tế, nhiều giải thuật học tăng cường **chính là những trường hợp đặc biệt** của SA. Ví dụ, thuật toán **Temporal-Difference (TD)** – một phương pháp phổ biến trong RL để dự báo giá trị – có thể được nhìn nhận như một dạng xấp xỉ ngẫu nhiên: TD(0) (phương pháp TD đơn giản nhất) là một trường hợp riêng của phương pháp SA tổng quát ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=The%20tabular%20TD,displaystyle%20%5Cpi)). Tương tự, thuật toán **Q-learning** nổi tiếng về tối ưu hành động trong RL cũng có **cấu trúc lặp ngẫu nhiên** hoàn toàn phù hợp với khuôn khổ SA ([](https://www.mit.edu/~jnt/Papers/J052-94-jnt-q.pdf#:~:text=each%20new%20piece%20of%20information,of%20parallel%20asynchronous%20algorithms%2C%20to)). Mỗi bước Q-learning điều chỉnh giá trị Q gần hơn đến giá trị mục tiêu dựa trên phần thưởng quan sát và ước lượng hiện tại – các “điều chỉnh ngẫu nhiên” này về bản chất tuân theo quy tắc cập nhật của SA.

Tại sao SA lại quan trọng với RL? Bởi lẽ trong RL, ta thường không thể tính trực tiếp giá trị kỳ vọng (ví dụ, giá trị kỳ vọng của phần thưởng tương lai) do môi trường phức tạp hoặc không biết mô hình. Thay vào đó, agent **học từ mẫu**: nó ước lượng giá trị hàm mục tiêu (như hàm giá trị, hàm Q hoặc gradient của hàm mục tiêu) qua từng trải nghiệm. Quá trình này chính là *học theo kiểu xấp xỉ ngẫu nhiên*: dần dần điều chỉnh các tham số nhằm hội tụ đến giá trị thật mong muốn. Lý thuyết SA cung cấp các công cụ toán học để phân tích sự hội tụ và ổn định của những cập nhật ngẫu nhiên như vậy, do đó **đảm bảo nền tảng** cho việc hiểu và chứng minh tính đúng đắn của các thuật toán RL. Chẳng hạn, các chứng minh hội tụ kinh điển cho Q-learning đều dựa trên việc xem thuật toán này như một trường hợp của SA và áp dụng các định lý hội tụ tương ứng ([](https://www.mit.edu/~jnt/Papers/J052-94-jnt-q.pdf#:~:text=each%20new%20piece%20of%20information,of%20parallel%20asynchronous%20algorithms%2C%20to)) ([Asynchronous Stochastic Approximation and Q-Learning | Machine Learning
        ](https://link.springer.com/article/10.1023/A:1022689125041#:~:text=We%20provide%20some%20general%20results,more%20general%20than%20previously%20available)). Tóm lại, SA vừa là **cơ sở lý thuyết**, vừa là **phương pháp triển khai thực tiễn** cho RL: nó mang lại khả năng học trực tuyến (online learning) từ dữ liệu nhiễu – điều cốt yếu làm nên sức mạnh của học tăng cường.

## Các thuật toán SA kinh điển: Robbins–Monro và Kiefer–Wolfowitz
Trong phần này, chúng ta điểm qua hai thuật toán SA kinh điển đặt nền móng cho lĩnh vực: **thuật toán Robbins–Monro (1951)** và **thuật toán Kiefer–Wolfowitz (1952)** ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=The%20earliest%2C%20and%20prototypical%2C%20algorithms,respectively%20in%201951%20and%201952)).

### Thuật toán Robbins–Monro (1951)
Robbins–Monro giải quyết bài toán sau: **tìm nghiệm $\theta^*$ sao cho $M(\theta^*) = \alpha$**, với $M(\theta) = \mathbb{E}[N(\theta)]$ là một hàm kỳ vọng mà ta không quan sát trực tiếp được ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=The%20Robbins%E2%80%93Monro%20algorithm%2C%20introduced%20in,can%20instead%20obtain%20measurements%20of)). Thay vào đó, ta có thể thực hiện các phép đo ngẫu nhiên $N(\theta)$ (có kỳ vọng đúng bằng $M(\theta)$) tại các giá trị $\theta$ tùy chọn. Nói cách khác, mục tiêu là giải phương trình $M(\theta) = \alpha$ khi chỉ biết **các quan sát nhiễu** $N(\theta)$ sao cho $\mathbb{E}[N(\theta)] = M(\theta)$.

Robbins và Monro đề xuất một thuật toán lặp để xấp xỉ nghiệm $\theta^*$. Ký hiệu $\theta_n$ là xấp xỉ tại bước $n$, thuật toán cập nhật theo công thức:

$$
\theta_{n+1} \;=\; \theta_n \;-\; a_n\,\big( N(\theta_n) - \alpha \big) \,,
$$

trong đó $a_n$ là bước kích thước (learning rate) dương và giảm dần ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Image%3A%20,generate%20iterates%20of%20the%20form)). Trực giác của cập nhật này: ở mỗi bước, ta đo giá trị ngẫu nhiên $N(\theta_n)$ (ví dụ như đầu ra nhiễu của một thí nghiệm tại $\theta_n$). Sai số $(N(\theta_n) - \alpha)$ cho biết $\theta_n$ đang nằm phía trên hay dưới nghiệm mục tiêu $\theta^*$ (vì $\mathbb{E}[N(\theta^*)] = \alpha$). Thuật toán hiệu chỉnh lại $\theta$ bằng cách *trừ đi* một phần sai số này (nhân với $a_n$) – do đó nếu $N(\theta_n)$ lớn hơn $\alpha$ (hàm M cho kết quả trên mức cần), $\theta$ sẽ giảm; ngược lại nếu nhỏ hơn, $\theta$ sẽ tăng. Dần dần, kỳ vọng của hiệu chỉnh tiến tới 0, và $\theta_n$ hy vọng hội tụ về $\theta^*$. 

Một cách nhìn khác: Robbins–Monro tìm *nghiệm gốc* của phương trình $f(\theta) = 0$ với $f(\theta) := M(\theta) - \alpha$. Cập nhật trên tương đương:

$$
\theta_{n+1} = \theta_n - a_n\, Y_n, \text{ với } Y_n := M(\theta_n) - \alpha + \varepsilon_n\,,
$$ 

ở đây $\varepsilon_n$ là nhiễu có kỳ vọng 0 (do $E[Y_n|\theta_n] = M(\theta_n) - \alpha$). Dạng này chính là công thức tổng quát của một bước SA: $\theta \leftarrow \theta - a_n [\text{quan sát nhiễu của }f(\theta)]$. Nếu lựa chọn các $a_n$ phù hợp (vd: $a_n = 1/n$), Robbins–Monro chứng minh được $\theta_n$ hội tụ ít nhất theo nghĩa xác suất đến $\theta^*$ ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=The%20Robbins%E2%80%93Monro%20algorithm%2C%20introduced%20in,can%20instead%20obtain%20measurements%20of)). Sau đó, các nhà toán học khác (như Blum) đã bổ sung điều kiện và chỉ ra thêm rằng thuật toán **hội tụ gần như chắc chắn** (với xác suất 1) đến nghiệm thực sự ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=that%20Image%3A%20,is%20actually%20with%20probability%20one)) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=every%20%7D%7D0)) (chi tiết hội tụ sẽ bàn thêm ở phần sau).

Một trường hợp cụ thể đơn giản giúp hiểu Robbins–Monro là **bài toán ước lượng trung bình**. Giả sử ta muốn ước lượng giá trị trung bình $\mu$ của một phân phối, nhưng ta chỉ có thể lấy mẫu $Z_n$ từ phân phối đó. Ta có thể đặt bài toán vào khung Robbins–Monro bằng cách chọn $M(\theta) = \mu - \theta$. Lúc này phương trình $M(\theta) = 0$ tương ứng $\theta = \mu$. Thuật toán Robbins–Monro trở thành:

- $Y_n = \phi(\theta_{n-1}) + \varepsilon_n = (\mu - \theta_{n-1}) + (Z_n - \mu)$, trong đó $Z_n - \mu$ là nhiễu có kỳ vọng 0.
- Cập nhật: $\theta_n = \theta_{n-1} + a_n Y_n = \theta_{n-1} + a_n (Z_n - \theta_{n-1})$.

Nếu chọn $a_n = \frac{1}{n}$, công thức trên chính là cách tính trung bình mẫu dần dần: $\theta_n$ bằng trung bình cộng của $n$ quan sát đầu tiên. Thuật toán đảm bảo $\theta_n \to \mu$ khi $n \to \infty$ (luật số lớn). Với $a_n$ tổng quát thỏa điều kiện hội tụ, ta vẫn có $\theta_n$ hội tụ về $\mu$ gần như chắc chắn. Đây chính là ví dụ minh họa Robbins–Monro trên bài toán đơn giản: **cập nhật trung bình từng phần tử một** cũng là một thuật toán xấp xỉ ngẫu nhiên.

### Thuật toán Kiefer–Wolfowitz (1952)
Trong khi Robbins–Monro giải bài toán nghiệm cho hàm kỳ vọng, **Kiefer–Wolfowitz** (K-W) mở rộng ý tưởng SA sang bài toán **tối ưu hàm mục tiêu**. Bài toán đặt ra: tìm $\theta$ tối ưu (cực đại hoặc cực tiểu) của một hàm $M(x)$, nhưng ta không trực tiếp quan sát được $M(x)$ mà chỉ có thể thực hiện các phép đo nhiễu $N(x)$ sao cho $\mathbb{E}[N(x)] = M(x)$ tại các điểm $x$ tùy chọn ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Let%20Image%3A%20,the%20iterates%20being%20generated%20as)). Nói cách khác, mục tiêu là tìm $\theta$ tối ưu của $M(x)$ thông qua các đánh giá $N(x)$ có nhiễu.

Thuật toán Kiefer–Wolfowitz áp dụng ý tưởng **gradient descent (hạ dốc)** trong bối cảnh ngẫu nhiên: tại mỗi bước, nó ước lượng gradient của hàm $M$ một cách ngẫu nhiên, rồi bước một bước nhỏ theo hướng gradient đó để cải thiện xấp xỉ $\theta$. Cụ thể, K-W sử dụng *phương pháp sai phân hữu hạn* để xấp xỉ gradient. Tại mỗi bước $n$, thuật toán thực hiện hai phép đo $N$ tại hai điểm gần $\theta_n$: một điểm tiến và một điểm lùi trên mỗi tọa độ, rồi lấy hiệu để ước lượng đạo hàm. Với trường hợp một chiều, công thức cập nhật của Kiefer–Wolfowitz có thể viết như sau ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=gradient,being%20generated%20as)):

$$
x_{n+1} = x_n + a_n \cdot \frac{N(x_n + c_n) - N(x_n - c_n)}{2\,c_n} \,,
$$

trong đó $a_n$ là bước kích thước (giảm dần theo $n$ như Robbins–Monro) và $c_n$ là độ dịch chuyển nhỏ (cũng giảm dần) dùng để lấy sai phân ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Image%3A%20%7B%5Cdisplaystyle%20x_%7Bn%2B1%7D%3Dx_%7Bn%7D%2Ba_%7Bn%7D%5Ccdot%20%5Cleft%28%7B%5Cfrac%20%7BN%28x_%7Bn%7D%2Bc_%7Bn%7D%29)). Biểu thức $\frac{N(x_n + c_n) - N(x_n - c_n)}{2c_n}$ chính là xấp xỉ đạo hàm của $M(x)$ tại $x_n$ (theo phương pháp vi phân trung tâm), với sai số ngẫu nhiên do $N$ mang lại. Thuật toán K-W tiến hành *“bước SA”* bằng cách dịch chuyển $x$ ngược dấu gradient ước lượng này để tăng giá trị kỳ vọng (đối với bài toán cực đại) hoặc giảm (đối với cực tiểu).

Kiefer và Wolfowitz đã chứng minh rằng, nếu hàm mục tiêu $M(x)$ thỏa mãn một số điều kiện đều đặn (ví dụ: có duy nhất một cực đại toàn cục và tính chất concave đủ mạnh cục bộ) và nếu chọn chuỗi tham số phù hợp (ví dụ: $a_n = 1/n$, $c_n \sim n^{-1/3}$ như họ gợi ý ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=A%20suitable%20choice%20of%20sequences%2C,1%2F3))), thì $x_n$ **hội tụ với xác suất 1** đến điểm tối ưu $\theta^*$ của $M(x)$ ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Kiefer%20and%20Wolfowitz%20proved%20that%2C,almost%20surely%2C%20provided%20that)) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=,domain%2C%20Kiefer%20and%20Wolfowitz%20proposed)). Điều kiện “strong concavity” về trực giác đảm bảo rằng hàm $M(x)$ có hình dạng “mõm chảo” xung quanh cực đại – do đó bước đi theo gradient sẽ dẫn đến điểm cao nhất.

Một hạn chế của phương pháp K-W nguyên bản là **chi phí tính toán**: với hàm mục tiêu nhiều chiều (dimension $d$), mỗi lần cập nhật gradient đòi hỏi $2d$ phép tính $N(x)$ (mỗi chiều 2 điểm). Điều này trở nên đắt đỏ khi $d$ lớn, làm chậm tốc độ hội tụ thực tế ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=1,This)). Để khắc phục, các nghiên cứu sau này đã đề xuất cải tiến. Đáng chú ý là **thuật toán SPSA (Simultaneous Perturbation Stochastic Approximation)** của Spall (1992), cho phép ước lượng gradient toàn cục chỉ với **2 lần đánh giá hàm** mỗi bước bất kể chiều của $x$ ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Kiefer%E2%80%93Wolfowitz%20algorithm%20will%20require%20substantial,method%20would%20require%20only%20two)). SPSA đạt được điều đó bằng cách nhiễu đồng thời tất cả các chiều của $\theta_n$ theo một hướng ngẫu nhiên duy nhất thay vì từng chiều tách biệt – từ đó giảm mạnh số lượng phép đo cần thiết mà vẫn đảm bảo hội tụ. SPSA là một ví dụ cho thấy sự phát triển tiếp nối của Kiefer–Wolfowitz: mở rộng tính hiệu quả để áp dụng cho các bài toán tối ưu quy mô lớn hiện đại.

Tóm lại, Robbins–Monro và Kiefer–Wolfowitz là hai trụ cột khởi đầu của xấp xỉ ngẫu nhiên. Robbins–Monro đặt nền tảng cho các thuật toán **học trung bình và giải phương trình sai phân kỳ vọng** (sau này liên quan đến các phương pháp TD trong RL), còn Kiefer–Wolfowitz mở đường cho **các phương pháp tối ưu ngẫu nhiên** (liên hệ đến các giải thuật gradient descent ngẫu nhiên trong học máy). Cả hai phương pháp đều thể hiện triết lý chung của SA: sử dụng **thông tin nhiễu** từ các phép thử để dần dần “leo” đến giá trị đúng, thay vì yêu cầu thông tin hoàn hảo ngay từ đầu.

## Liên hệ giữa SA và cập nhật giá trị trong RL
Một trong những kết nối quan trọng nhất giữa xấp xỉ ngẫu nhiên và học tăng cường nằm ở **các thuật toán cập nhật giá trị** của RL, như phương pháp TD, Q-learning, SARSA, v.v. Tất cả các thuật toán này đều có dạng cập nhật lặp tương tự như công thức SA: giá trị mới = giá trị cũ + (hệ số học) × (tín hiệu lỗi ngẫu nhiên). Ta xét hai ví dụ tiêu biểu:

- **Temporal-Difference Learning (TD)**: Trong bài toán **đánh giá giá trị trạng thái** (policy evaluation), ta muốn ước lượng hàm giá trị $V^\pi(s)$ thỏa mãn phương trình Bellman kỳ vọng: $V^\pi(s) = \mathbb{E}_\pi\{R_1 + \gamma V^\pi(S_1) \mid S_0=s\}$. Thuật toán TD(0) cập nhật xấp xỉ $V$ theo mẫu $(S_t, R_{t+1}, S_{t+1})$ như sau: 
  $$V_{new}(S_t) \leftarrow V_{old}(S_t) + \alpha\,\big[ R_{t+1} + \gamma V_{old}(S_{t+1}) - V_{old}(S_t) \big]\,.$$ 
  Đây chính là một bước **Robbins–Monro**: phần trong ngoặc vuông là **sai số TD** – đại lượng ngẫu nhiên có kỳ vọng đúng bằng 0 nếu $V_{old}$ trùng với giá trị thật $V^\pi$ ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=We%20drop%20the%20action%20from,Bellman%20Equation)). Thật vậy, $\mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t] = V^\pi(S_t)$, nên khi $V_{old}=V^\pi$, kỳ vọng của **tín hiệu điều chỉnh** $Z = R_{t+1} + \gamma V_{old}(S_{t+1}) - V_{old}(S_t)$ bằng 0. Do đó, theo lý thuyết SA, dãy $V_n$ sẽ hội tụ về giá trị đúng $V^\pi$ dưới các điều kiện thích hợp. TD(0) chính là **trường hợp đặc biệt** của xấp xỉ ngẫu nhiên cho bài toán tìm điểm cố định $V = \mathcal{T}^\pi(V)$, trong đó $\mathcal{T}^\pi$ là toán tử Bellman theo chính sách $\pi$ (một ánh xạ co với hệ số $\gamma$). Kết quả là, *đảm bảo hội tụ* của TD(0) có thể suy ra từ định lý hội tụ của SA cho ánh xạ co ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=The%20tabular%20TD,displaystyle%20%5Cpi)).

- **Q-learning**: Trong bài toán **tối ưu hành động**, Q-learning tìm hàm hành động $Q^*(s,a)$ thỏa mãn phương trình Bellman tối ưu: $Q^*(s,a) = \mathbb{E}\{R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') \mid S_t=s, A_t=a\}$. Thuật toán Q-learning cập nhật theo mẫu $(S_t, A_t, R_{t+1}, S_{t+1})$:
  $$Q_{new}(S_t, A_t) \leftarrow Q_{old}(S_t, A_t) + \alpha\,\big[ R_{t+1} + \gamma \max_{a'} Q_{old}(S_{t+1}, a') - Q_{old}(S_t, A_t) \big]\,.$$ 
  Tương tự TD, biểu thức trong ngoặc vuông là **sai số TD** cho Q-learning – nó đo chênh lệch giữa giá trị Q hiện tại và giá trị mục tiêu $R+\gamma \max Q$ một bước. Khi $Q_{old}$ trùng với $Q^*$, kỳ vọng của sai số này (có điều kiện trên trạng thái–hành động hiện tại) bằng 0, vì $Q^*$ thỏa mãn phương trình cố định $Q^* = \mathcal{T}^*(Q^*)$. Do đó, Q-learning cũng thuộc dạng SA để tìm **điểm cố định** của toán tử Bellman tối ưu $\mathcal{T}^*$. Toán tử này là một ánh xạ co ($\gamma < 1$ đảm bảo điều đó), nên mặc dù công thức Q-learning không tuyến tính (do toán tử $\max$), vẫn có thể áp dụng lý thuyết SA và ánh xạ co để chứng minh hội tụ. Thật vậy, Tsitsiklis (1994) đã sử dụng kết hợp lý thuyết xấp xỉ ngẫu nhiên và tính chất co để chứng minh Q-learning hội tụ tới $Q^*$ với xác suất 1, với điều kiện tất cả các cặp trạng thái-hành động được thăm dò đủ thường xuyên và $\alpha_n$ thỏa mãn tiêu chuẩn giảm dần ([](https://www.mit.edu/~jnt/Papers/J052-94-jnt-q.pdf#:~:text=each%20new%20piece%20of%20information,of%20parallel%20asynchronous%20algorithms%2C%20to)) ([Reinforcement Learning](https://remydegenne.github.io/docs/SL_2022/Cours3.pdf#:~:text=Both%20aim%20at%20learning%20the,32)). Nói một cách trực quan, Q-learning xem *mỗi trải nghiệm* như một phép đo nhiễu của giá trị tối ưu cần đạt được, rồi dùng nguyên tắc Robbins–Monro để “điều chỉnh” dần hàm Q tới khi không còn sai số.

Ngoài hai ví dụ trên, mọi thuật toán RL cập nhật giá trị theo kiểu *cấp tiến dần dần từ mẫu* (incremental, sample-by-sample) đều có thể nhìn dưới lăng kính SA. Thuật toán **SARSA** (on-policy TD control) cũng chỉ khác Q-learning ở chỗ dùng hành động kế tiếp theo chính sách thay vì max, nhưng cấu trúc cập nhật vẫn là $Q \leftarrow Q + \alpha [\text{sample} - Q]$. Phương pháp **actor-critic** trong RL cũng dựa trên SA hai tầng: *critic* cập nhật hàm giá trị theo TD (một SA nhanh), trong khi *actor* cập nhật chính sách dựa trên gradient thu được từ critic (một SA chậm hơn) – ta sẽ đề cập thêm ở phần biến thể hiện đại. Ngay cả các phương pháp **chính sách gradient** (policy gradient) cũng là SA: ở mỗi bước, gradient của hàm mục tiêu (ví dụ kỳ vọng thưởng) được ước lượng bằng mẫu (dựa trên tập episode thu thập), rồi ta cập nhật tham số chính sách một bước nhỏ theo hướng gradient mẫu đó. Quá trình lặp lại chính là *stochastic gradient descent* – một dạng đặc biệt của xấp xỉ ngẫu nhiên.

Tóm lại, cập nhật giá trị trong học tăng cường **là một ứng dụng trực tiếp của nguyên lý xấp xỉ ngẫu nhiên**. Mỗi lần một agent điều chỉnh ước lượng từ dữ liệu trải nghiệm, nó đang thực hiện một bước của thuật toán SA. Nhờ vậy, các đảm bảo lý thuyết về hội tụ của SA (với điều kiện thích hợp) cũng mang lại sự đảm bảo cho các giải thuật RL tương ứng ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=The%20tabular%20TD,displaystyle%20%5Cpi)) ([](https://www.mit.edu/~jnt/Papers/J052-94-jnt-q.pdf#:~:text=each%20new%20piece%20of%20information,of%20parallel%20asynchronous%20algorithms%2C%20to)). Sự gắn kết chặt chẽ này giải thích vì sao việc nghiên cứu SA có ý nghĩa nền tảng đối với RL cả về mặt lý thuyết lẫn thực tiễn.

## Các điều kiện hội tụ quan trọng và ý nghĩa toán học
Thuật toán xấp xỉ ngẫu nhiên, do bản chất sử dụng mẫu ngẫu nhiên, cần những điều kiện nhất định để đảm bảo hội tụ đến kết quả đúng. Dưới đây là một số điều kiện hội tụ quan trọng thường được yêu cầu trong lý thuyết SA, kèm theo ý nghĩa trực quan:

1. **Bước kích thước giảm dần phù hợp** – Ký hiệu $\{\alpha_n\}$ là chuỗi bước cập nhật (learning rate) theo mỗi vòng lặp. Điều kiện kinh điển là: 
   $$\sum_{n=0}^{\infty} \alpha_n = \infty, \qquad \sum_{n=0}^{\infty} \alpha_n^2 < \infty.$$ 
   Điều này có nghĩa $\alpha_n$ phải giảm về 0 đủ chậm (tổng vô hạn) nhưng không quá chậm (tổng bình phương hữu hạn) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=C2%29%20Image%3A%20%7B%5Cdisplaystyle%20%5Csum%20_%7Bn%3D0%7D,n%7D%3D%5Cinfty)). Ý nghĩa: tổng vô hạn đảm bảo thuật toán có “đủ thời gian” để đi đến đích – nếu $\sum \alpha_n < \infty$ thì tổng bước di chuyển bị hữu hạn, thuật toán có thể dừng tiến trước khi chạm tới nghiệm (tưởng tượng nếu các bước ngày càng nhỏ quá nhanh, thuật toán “khựng lại” giữa chừng) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Here%20are%20some%20intuitive%20explanations,displaystyle%20%5Ctheta)). Ngược lại, tổng bình phương hữu hạn đảm bảo độ nhấp nhô của bước không quá lớn – nếu $\alpha_n$ giảm quá chậm, nhiễu ngẫu nhiên có thể tích lũy và cản trở hội tụ (tương tự việc lắc lư mãi không tắt dần). Thông thường, lựa chọn $\alpha_n = 1/n$ (hoặc $1/(n+1)$) thỏa mãn tiêu chí này và được dùng phổ biến ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Image%3A%20%7B%5Cdisplaystyle%20%5Ctheta%20_%7Bn%2B1%7D,search%20direction%20of%20the%20algorithm)).

2. **Nhiễu có kỳ vọng bằng 0 và phương sai hữu hạn** – Giả sử thuật toán SA của ta có dạng tổng quát: $x_{n+1} = x_n + \alpha_n [H(x_n, \xi_n)]$, trong đó $\xi_n$ đại diện cho biến ngẫu nhiên (nguồn nhiễu) ở bước $n$ và $H(x, \xi)$ là cập nhật đơn bước. Điều kiện cần thiết là $H(x^*, \xi)$ có kỳ vọng bằng 0 tại nghiệm $x^*$ (tức bước cập nhật không sai lệch trung bình khi đến đúng nghiệm). Ngoài ra, cần giả sử nhiễu có phương sai bị chặn (hoặc ít nhất là **quyển kiểm soát** được) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=,domain%2C%20Kiefer%20and%20Wolfowitz%20proposed)). Điều này hợp lý: nếu nhiễu có phương sai vô hạn hoặc có thiên lệch ngay cả ở nghiệm đúng, ta khó lòng đòi hỏi hội tụ chính xác. Trong ngữ cảnh RL, điều kiện này thường tương ứng với giả thiết phần thưởng hữu hạn, mô hình Markov ổn định, và **mẫu quan sát không chệch** khi đã đạt giá trị tối ưu.  

3. **Hàm mục tiêu có tính chất ổn định (co hoặc convex) xung quanh nghiệm** – Lý thuyết hội tụ SA thường đòi hỏi hàm mục tiêu (ví dụ hàm $g(\theta)$ mà ta muốn giải $g(\theta)=0$ hoặc muốn cực tiểu) thỏa mãn một số điều kiện hình học như *đơn điệu mạnh* hoặc *convex mạnh* gần nghiệm. Chẳng hạn, Robbins–Monro giả sử hàm $M(\theta)$ đơn điệu tăng (để đảm bảo nghiệm duy nhất). Kiefer–Wolfowitz yêu cầu $M(x)$ có cực đại duy nhất và **strong concavity** quanh điểm đó ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=,domain%2C%20Kiefer%20and%20Wolfowitz%20proposed)). Những điều kiện này đảm bảo **tính duy nhất và hấp dẫn** của nghiệm: nghiệm đúng là đích đến “hút” quỹ đạo thuật toán, và bất kỳ sai lệch nào sẽ tạo ra tín hiệu điều chỉnh kéo thuật toán quay về. Trong RL, tính chất co của *toán tử Bellman* (với hệ số $\gamma < 1$) đóng vai trò tương tự – nó đảm bảo phương trình cố định có nghiệm duy nhất và việc cập nhật theo Bellman (hay TD) sẽ hội tụ về nghiệm đó.

Khi các điều kiện trên (và một số điều kiện kỹ thuật khác như giới hạn độ lớn cập nhật $|H(x_n, \xi_{n+1})| \le B$ nào đó ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=C4%29%20Image%3A%20,B))) được thỏa mãn, ta có các định lý hội tụ kinh điển: **$\theta_n \to \theta^*$ gần như chắc chắn** khi $n \to \infty$ ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=every%20%7D%7D0)) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Kiefer%20and%20Wolfowitz%20proved%20that%2C,almost%20surely%2C%20provided%20that)). Nói cách khác, xác suất để thuật toán SA tiếp cận đúng nghiệm mục tiêu (hoặc tối ưu toàn cục) sẽ bằng 1. Hơn nữa, lý thuyết còn cho biết tốc độ hội tụ tiệm cận: thông thường sai số giảm tỷ lệ $O(1/\sqrt{n})$ (đối với độ lệch chuẩn của ước lượng) – đây là tốc độ hội tụ tối ưu không thể cải thiện hơn nữa dưới nhiễu ngẫu nhiên theo kết quả của Chung, Lai & Robbins ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=algorithm%20will%20achieve%20the%20asymptotically,textstyle)) ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=the%20asymptotically%20optimal%20convergence%20rate%2C,this%20rate%20cannot%20be%20improved)). Một cách cải thiện độ chính xác tiệm cận mà không vi phạm tốc độ trên là dùng **trung bình dạo** các iterates (Polyak–Ruppert averaging): thay vì lấy $\theta_n$ cuối làm kết quả, ta lấy trung bình cộng các $\theta_i$ từ một điểm nào đó đến $n$. Phương pháp này không thay đổi điểm hội tụ nhưng có thể giảm phương sai, giúp sai số bình phương giảm cỡ $O(1/n)$ trong giá trị hàm mục tiêu ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=algorithm%20will%20achieve%20the%20asymptotically,textstyle)) – một kỹ thuật hay được áp dụng trong thực nghiệm để làm mượt kết quả.

Về mặt toán học, một công cụ quan trọng để phân tích hội tụ của SA là **phương pháp phương trình vi phân tương đương (ODE method)** ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=used,therefore%2C%20has%20several%20results%20of)). Ý tưởng: ta xem quá trình rời rạc $\theta_n$ là mẫu lấy rời rạc của nghiệm một phương trình vi phân $d\theta/dt = h(\theta)$ nào đó (phụ thuộc vào hàm mục tiêu). Khi $n$ lớn (bước nhỏ), quỹ đạo $\theta_n$ sẽ xấp xỉ đường cong nghiệm ODE. Nếu ODE hội tụ đến điểm cân bằng duy nhất, thì thuật toán SA cũng sẽ hội tụ về điểm đó với xác suất 1 ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=used,therefore%2C%20has%20several%20results%20of)). Phương pháp ODE cho phép chứng minh hội tụ một cách trực quan (nhiễu dần “trung bình hóa” thành dòng trơn) và cũng cho phép phân tích các trường hợp phức tạp như **nhiễu Markov** (trong RL, các quan sát liên tiếp phụ thuộc nhau qua trạng thái Markov). Borkar & Meyn (2000) và Kushner & Yin (1997) là những tài liệu kinh điển áp dụng phương pháp ODE/differential inclusion để chứng minh hội tụ cho nhiều thuật toán RL. Hiện nay, ngoài hội tụ tiệm cận, người ta còn quan tâm đến **phân tích hữu hạn** (finite-sample analysis) cho SA – tức là đánh giá tốc độ hội tụ sau một số hữu hạn bước (ví dụ với xác suất cao). Đây là hướng nghiên cứu hiện đại đang phát triển, nhất là trong bối cảnh RL phức tạp (như hàm giá trị xấp xỉ, hai khoảng thời gian) ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=Abstract%20Two,new%20projection%20scheme%2C%20in%20which)) ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=Reinforcement%20Learn%02ing%20%28RL%29,This%20scheme%20allows%20one%20to)).

Tóm lại, để một thuật toán SA (và RL) hội tụ đáng tin cậy, ta cần lựa chọn **tốc độ học phù hợp**, đảm bảo **nhiễu quan sát không chệch/quá lớn**, và **bài toán có cấu trúc tốt** (ví dụ tính co hoặc convex đảm bảo duy nhất nghiệm). Khi đó, các đảm bảo hội tụ mạnh (như hội tụ gần như chắc chắn) sẽ có hiệu lực, và ta có thể tự tin rằng thuật toán sẽ tìm ra lời giải đúng nếu chạy đủ lâu.

## Các mở rộng và biến thể hiện đại của SA
Kể từ thời Robbins–Monro, lý thuyết xấp xỉ ngẫu nhiên đã có nhiều mở rộng và biến thể để thích ứng với các bài toán đa dạng và yêu cầu của thời đại mới (như tối ưu quy mô lớn, học sâu, v.v.). Dưới đây là một số hướng phát triển và biến thể hiện đại đáng chú ý của SA:

- **Thuật toán SPSA (Simultaneous Perturbation Stochastic Approximation)**: Như đã đề cập, SPSA do J. Spall đề xuất giúp giảm đáng kể số lần đánh giá hàm cho mỗi bước cập nhật trong bài toán tối ưu không có gradient. Thay vì tính sai phân từng chiều như Kiefer–Wolfowitz (cần $2d$ phép đo cho không gian $d$ chiều), SPSA mỗi bước chỉ cần **2 phép đo**: nó tạo một vector nhiễu ngẫu nhiên $\Delta_n \in \mathbb{R}^d$, sau đó ước lượng gradient $\nabla M(x_n) \approx \frac{N(x_n + c_n \Delta_n) - N(x_n - c_n \Delta_n)}{2c_n} \Delta_n^{-1}$ (theo thành phần) và cập nhật $x_{n+1} = x_n + a_n \,\hat{\nabla} M(x_n)$ ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Kiefer%E2%80%93Wolfowitz%20algorithm%20will%20require%20substantial,method%20would%20require%20only%20two)). Bằng cách nhiễu đồng thời tất cả hướng, SPSA đạt hiệu quả cao trong các bài toán điều chỉnh tham số nhiều chiều (ví dụ tinh chỉnh tham số hệ thống, điều khiển robots) và đã được áp dụng thành công trong cả lĩnh vực RL khi hàm mục tiêu không có biểu thức đạo hàm rõ ràng.

- **Thuật toán hai khoảng thời gian (Two-timescale SA)**: Nhiều bài toán học tăng cường hiện đại liên quan đến việc cập nhật **hai (hoặc nhiều) nhóm tham số** với tốc độ khác nhau. Ví dụ, trong phương pháp **actor-critic**, *critic* (đánh giá giá trị) được cập nhật nhanh, còn *actor* (chính sách) cập nhật chậm hơn; hay trong các giải thuật **Gradient TD** (GTDe) để ước lượng giá trị với hàm xấp xỉ tuyến tính, có hai chuỗi tham số song song. Lý thuyết SA hai khoảng thời gian xử lý tình huống này bằng cách xét $\theta_n$ và $\phi_n$ với bước $\alpha_n$ và $\beta_n$ sao cho $\alpha_n$ giảm nhanh hơn (nhóm $\theta$ “nhanh”, nhóm $\phi$ “chậm”). Khi $n$ lớn, $\theta_n$ có thể coi như hội tụ tức thời so với $\phi_n$, và ta chứng minh được cặp $(\theta_n,\phi_n)$ hội tụ đến điểm cân bằng mong muốn. Các thuật toán hai tầng xuất hiện rộng rãi trong RL (actor-critic, API/ACE, Off-policy TD với correction v.v.), và lý thuyết gần đây đã cung cấp kết quả hội tụ cùng phân tích độ lệch chuẩn cho chúng ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=Abstract%20Two,new%20projection%20scheme%2C%20in%20which)) ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=points%20or%20zeros%20of%20a,when%20function%20approximation%20is%20used)). Chẳng hạn, Konda & Tsitsiklis (2003) đã chứng minh hội tụ của actor-critic hai tầng bằng cách áp dụng SA hai khoảng thời gian. Hiện nay cũng có các kết quả **phân tích hữu hạn** cho SA hai tầng trong một số trường hợp (Dalal et al. 2018) ([](http://proceedings.mlr.press/v75/dalal18a/dalal18a.pdf#:~:text=Abstract%20Two,new%20projection%20scheme%2C%20in%20which)), cung cấp hướng dẫn thực tiễn về cách chọn tỷ lệ tốc độ học giữa hai tầng.

- **Trung bình hóa Polyak–Ruppert và các kỹ thuật giảm phương sai**: Biến thể này đã được nhắc đến ở trên – thay vì dùng giá trị cuối cùng, ta trung bình các giá trị theo thời gian để giảm phương sai. Điều này đặc biệt hữu ích khi kết hợp với bước kích thước cố định (hoặc giảm chậm) – phương pháp thường gọi là **SA với bước cố định + trung bình dạo**, giúp đạt hiệu năng cao hơn trong số bước hữu hạn. Ngoài ra, trong tối ưu, còn có các kỹ thuật như **mini-batch SA** (lấy trung bình nhiều mẫu nhiễu mỗi bước thay vì một mẫu) – điều này tương đương giảm phương sai của cập nhật mỗi bước, đánh đổi bằng chi phí tính toán cao hơn. Trong bối cảnh RL, ví dụ, việc dùng **kinh nghiệm hồi tưởng (experience replay)** và cập nhật theo mini-batch trong Deep Q-Network có thể coi là một dạng “mini-batch SA”, giúp giảm nhiễu cập nhật và làm quá trình học ổn định hơn.

- **Các phương pháp tăng tốc (Momentum, Newton)**: Để cải thiện tốc độ hội tụ, các ý tưởng từ tối ưu quyết định cũng được đưa vào SA. **Thuật toán momentum SGD** (như áp dụng momentum, Adam, RMSProp trong huấn luyện mạng neuron) về nguyên tắc cũng là SA với cập nhật được hiệu chỉnh bởi một thành phần động lượng. Dù việc phân tích lý thuyết của chúng phức tạp hơn, trong thực nghiệm momentum giúp hội tụ nhanh và mượt hơn. Một hướng khác là **phương pháp Newton ngẫu nhiên**, tức sử dụng xấp xỉ ma trận Hessian nghịch đảo làm “hệ số khuếch đại ma trận” cho cập nhật Robbins–Monro, tương tự như bước Newton trong tối ưu. **Zap Q-learning** (2019) là một ví dụ: thuật toán này đưa thêm một ma trận được cập nhật đồng thời nhằm xấp xỉ nghịch đảo của Jacobian của toán tử Bellman, nhờ đó **tăng tốc** Q-learning bằng cách giảm phương sai tới mức tiệm cận tối thiểu ([Zap Q-Learning](http://papers.neurips.cc/paper/6818-zap-q-learning.pdf#:~:text=algorithms,case%20of%20a%20complete%20parameterization)). Zap Q-learning được xây dựng dưới dạng SA hai tầng (vừa học Q vừa học ma trận khuếch đại), mô phỏng phương pháp Newton-Raphson trong môi trường ngẫu nhiên, và kết quả cho thấy nó giảm đáng kể phương sai và cải thiện tốc độ hội tụ so với Q-learning chuẩn ([Zap Q-Learning](http://papers.neurips.cc/paper/6818-zap-q-learning.pdf#:~:text=Q,the%20associated%20ODE%20has%20a)) ([Zap Q-Learning](http://papers.neurips.cc/paper/6818-zap-q-learning.pdf#:~:text=of%20Watkins%E2%80%99%20algorithm%20,theory%20from%20extended%20version%20of)). Đây là minh chứng cho khả năng kết hợp ý tưởng tối ưu hóa thứ hai (như Hessian) vào SA.

- **SA với hàm xấp xỉ (Function Approximation)**: Trong RL hiện đại, trạng thái không còn nhỏ để có bảng tra cứu, thay vào đó ta dùng mô hình hàm xấp xỉ (như mạng neuron) để biểu diễn giá trị hoặc chính sách. Lý thuyết SA cũng đã mở rộng để bao quát trường hợp hàm xấp xỉ, nơi việc cập nhật trở nên phức tạp (do hàm mục tiêu có thể không còn co, thậm chí không còn hội tụ chính xác vì hàm xấp xỉ hữu hạn). Các thuật toán như **Gradient TD (GTD)**, **TDC** được phát triển để đảm bảo hội tụ với hàm xấp xỉ tuyến tính, bằng cách sử dụng cấu trúc hai tầng SA để xử lý trường hợp off-policy. Với mạng neuron phi tuyến (như trong Deep RL), lý thuyết SA cổ điển chưa đảm bảo hội tụ toàn cục, nhưng trong thực tế người ta vẫn áp dụng (ví dụ DQN, Policy Gradient) và quan sát kết quả thành công. Nhiều nghiên cứu đương đại đang cố gắng thu hẹp khoảng cách giữa thực nghiệm và lý thuyết, ví dụ tìm điều kiện để SA với hàm xấp xỉ phi tuyến hội tụ (gần đây có kết quả cho hội tụ local dưới điều kiện khởi tạo tốt, v.v.).

Tóm lại, các biến thể hiện đại của SA tập trung vào **cải thiện hiệu năng và mở rộng phạm vi áp dụng**: từ giảm chi phí tính toán (SPSA), xử lý nhiều tốc độ học (two-timescale), giảm phương sai (trung bình hóa, mini-batch), tăng tốc hội tụ (Newton, momentum), đến hỗ trợ mô hình hàm xấp xỉ phức tạp. Chính nhờ những tiến bộ này, SA tiếp tục là nền tảng vững chắc cho các thuật toán học máy và học tăng cường tiên tiến.

## Ứng dụng thực tiễn trong học tăng cường và các ví dụ minh họa
Phương pháp xấp xỉ ngẫu nhiên không chỉ có ý nghĩa lý thuyết mà còn **đóng vai trò then chốt trong các ứng dụng thực tiễn của học tăng cường**. Dưới đây, chúng ta xem xét một số tình huống và ví dụ cụ thể minh họa cho việc áp dụng SA trong RL:

- **Multi-armed Bandit (bài toán nhiều tay kéo)**: Đây là bài toán RL cơ bản không có trạng thái, trong đó một agent thử nghiệm các lựa chọn (các “tay kéo” máy đánh bạc) để tối đa hóa thưởng. Thuật toán epsilon-greedy cho bandit duy trì ước lượng giá trị trung bình cho mỗi hành động, thường cập nhật sau mỗi lần nhận thưởng: 
  $$Q_{new}(a) \leftarrow Q_{old}(a) + \alpha [R - Q_{old}(a)]\,.$$ 
  Công thức này chính là *Robbins–Monro* cho việc ước lượng kỳ vọng $\mathbb{E}[R|a]$ (giá trị trung bình của phần thưởng khi chọn hành động $a$). Thật vậy, nếu $Q_{old}(a)$ trùng với giá trị thật $q^*(a)$, thì kỳ vọng của tín hiệu điều chỉnh $R - Q_{old}(a)$ bằng 0. Do đó theo thời gian, $Q(a)$ sẽ hội tụ về $q^*(a)$ với xác suất 1, miễn là mỗi hành động $a$ được thăm dò vô hạn lần (đảm bảo điều kiện nhiễu đủ phong phú) và $\alpha$ giảm dần phù hợp. Trên thực tế, công thức trên có thể được hiện thực hóa bằng cách lấy $\alpha = 1/n_a$ (nghịch đảo số lần đã chọn hành động $a$), khi đó $Q(a)$ đúng bằng trung bình mẫu của các phần thưởng đã nhận của $a$. Bandit nhiều tay cho thấy cách SA cho phép agent **học giá trị kỳ vọng** của hành động từ phép thử ngẫu nhiên. Ứng dụng: từ tối ưu quảng cáo (học tỷ lệ click trung bình của các chiến dịch), cho tới tối ưu hóa lâm sàng (thử nghiệm các liều thuốc xem liều nào hiệu quả nhất trung bình) – tất cả đều có thể mô hình hóa dạng bandit và giải bằng SA.

- **Dự báo giá trị trong quy trình Markov**: Xét một ví dụ kinh điển từ sách của Sutton & Barto: bài toán **đi bộ ngẫu nhiên** (random walk) với 5 trạng thái trên một đoạn thẳng ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=The%20tabular%20TD,displaystyle%20%5Cpi)). Mục tiêu là dự báo xác suất chuỗi sẽ kết thúc ở phía phải (thắng) bắt đầu từ mỗi trạng thái. Phương pháp Monte Carlo sẽ chờ đến khi một episode kết thúc mới cập nhật giá trị dự báo. Ngược lại, phương pháp TD(0) áp dụng cập nhật SA ngay sau mỗi bước, sử dụng giá trị dự báo hiện tại của trạng thái kế tiếp để cập nhật trạng thái hiện tại. Kết quả đã được Sutton & Barto chỉ ra: TD(0) hội tụ nhanh hơn hẳn so với Monte Carlo, nhờ việc sử dụng thông tin sớm và cập nhật liên tục. Cụ thể, sau khoảng 100 episode, TD(0) cho ước lượng giá trị khá chính xác, trong khi Monte Carlo còn dao động mạnh. Điều này minh họa **ưu thế của SA** trong RL: nhờ cập nhật thường xuyên theo bước nhỏ, nó **hội tụ ổn định hơn** và tận dụng được dữ liệu hiệu quả hơn (mỗi bước đi có đóng góp ngay vào ước lượng, thay vì đợi đến cuối episode). Bài toán dự báo giá trị xuất hiện rộng rãi, từ ước lượng giá trị kỳ vọng của tình huống trong trò chơi (như dự đoán khả năng thắng cờ ở một thế cờ), đến dự báo dài hạn trong chuỗi quyết định (như giá trị kỳ vọng của trạng thái trong quản lý tài chính). Tất cả đều có thể giải bằng các thuật toán TD – tức là áp dụng SA – để thu được lời giải xấp xỉ.

- **Điều khiển tối ưu và tìm chính sách trong MDP**: Một ví dụ minh họa khác là **bài toán Cliff Walking** (đi dọc vách đá) nổi tiếng trong học tăng cường ([Reinforcement Learning](https://remydegenne.github.io/docs/SL_2022/Cours3.pdf#:~:text=I%20Q,st%2C%20at)) ([Reinforcement Learning](https://remydegenne.github.io/docs/SL_2022/Cours3.pdf#:~:text=Both%20aim%20at%20learning%20the,32)). Agent phải đi từ vị trí xuất phát đến mục tiêu trên một lưới ô vuông, với phần thưởng -1 mỗi bước và -100 nếu rơi xuống mép vực (cliff). Thuật toán Q-learning (off-policy) và SARSA (on-policy) đều có thể áp dụng. Cả hai sẽ học giá trị Q(s,a) cho mỗi trạng thái và hành động bằng cách cập nhật SA dựa trên trải nghiệm. Kết quả sau quá trình học: Q-learning tìm được **chính sách tối ưu** (đi sát mép vực để nhanh đến đích nhất, mặc dù nguy hiểm), trong khi SARSA học được **chính sách an toàn hơn** (đi vòng xa vực, tránh nguy cơ rơi, do tính on-policy nên nó học theo hành vi thám hiểm của mình). Thí nghiệm cho thấy Q-learning cuối cùng đạt giá trị tích lũy tốt hơn (vì tìm đúng chính sách tối ưu), nhưng trong quá trình học nó gặp nhiều phần thưởng âm do rơi vực; còn SARSA trong quá trình học nhận thưởng cao hơn (ít rơi hơn) nhưng hội tụ đến chính sách không tối ưu hẳn. Sự khác biệt này bắt nguồn từ cách **SA xử lý thông tin mẫu**: Q-learning mỗi bước cập nhật hướng tới ước lượng tốt nhất của mình (target policy tối ưu), còn SARSA cập nhật hướng tới hành động mà nó *thực sự* sẽ làm tiếp (policy hiện tại). Cả hai đều tận dụng SA để dần cải thiện, nhưng khác nhau ở cách sử dụng dữ liệu off-policy hay on-policy. Bài toán Cliff Walking là minh chứng rõ ràng về cách tiếp cận SA giúp agent **học hỏi từ tương tác môi trường và cải thiện chính sách dần dần**. Trong thực tế, các hệ thống robot hoặc điều khiển tự động có thể dùng những thuật toán như vậy: ví dụ một robot di chuyển trong nhà có thể dùng Q-learning để học cách đi đến trạm sạc nhanh nhất (dù có thể phải thử các đường nguy hiểm), hoặc dùng SARSA nếu muốn trong quá trình học tránh va chạm nhiều.

- **Học tăng cường sâu (Deep Reinforcement Learning)**: Những tiến bộ ngoạn mục trong học tăng cường những năm gần đây – từ việc **chơi trò Atari đạt hoặc vượt trình độ con người**, đến **AlphaGo/AlphaZero chiến thắng con người ở cờ vây và cờ vua**, hay **AlphaStar đánh bại cao thủ StarCraft** – đều được xây dựng trên nền tảng các thuật toán RL truyền thống kết hợp với mô hình hàm xấp xỉ (mạng neural) sâu. Mặc dù có nhiều kỹ thuật phức tạp, yếu tố cốt lõi là: **các thuật toán SA vẫn nằm ở trung tâm quá trình học**. Ví dụ, thuật toán **Deep Q-Network (DQN)** sử dụng Q-learning kết hợp mạng neuron để xấp xỉ hàm $Q(s,a)$. Mạng neuron này được huấn luyện bằng cách **minimize lỗi TD** qua mỗi mini-batch trải nghiệm, về thực chất chính là một dạng stochastic gradient descent (SA) trong không gian tham số của mạng ([Stochastic approximation - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_approximation#:~:text=Recently%2C%20stochastic%20approximations%20have%20found,2)). Tương tự, **AlphaGo Zero** sử dụng thuật toán Monte Carlo Tree Search kết hợp với **Temporal-Difference learning** để cập nhật mạng giá trị và chính sách dựa trên kết quả tự chơi – đây cũng là một quá trình SA trên tham số mạng, với mỗi ván chơi cung cấp các **mẫu huấn luyện ngẫu nhiên**. Những thành tựu này chứng tỏ SA có khả năng mở rộng và phối hợp với mô hình phức tạp: dù mạng neural có hàng triệu tham số, việc cập nhật vẫn tuân theo nguyên lý “tham số = tham số + bước học × gradient ngẫu nhiên” – nền tảng của SA. Chính nhờ hàng triệu bước cập nhật nhỏ nhặt như vậy mà hệ thống dần dần **tích lũy tri thức** và đạt được kỹ năng cao. Trong ứng dụng thực tế, các bài toán như **điều khiển robot**, **ô tô tự hành**, **quản lý năng lượng thông minh**, **tối ưu hóa hệ thống viễn thông**… hiện nay cũng ứng dụng học tăng cường sâu. Ở cốt lõi, mỗi lần robot điều chỉnh nhẹ tham số điều khiển sau một lần thử, hay hệ thống năng lượng cập nhật chính sách theo phản hồi tiêu thụ, thì một **bước xấp xỉ ngẫu nhiên** đang diễn ra.

Kết lại, **xấp xỉ ngẫu nhiên hiện diện trong hầu hết mọi ứng dụng thực tiễn của học tăng cường** – từ ví dụ đơn giản nhất đến hệ thống phức tạp nhất. Nhờ SA, các agent có thể học hỏi từ môi trường thực tế đầy nhiễu và bất định, cải thiện hiệu suất theo thời gian. Sự thành công của các hệ thống RL tiên tiến ngày nay là minh chứng sống động cho sức mạnh của nguyên lý SA: *tích lũy dần dần kiến thức từ dữ liệu ngẫu nhiên có thể dẫn đến kết quả vượt trội mà không cần mô hình hóa tường minh môi trường*. Điều này củng cố tầm quan trọng của việc hiểu và vận dụng SA trong việc phát triển các thuật toán học tăng cường hiệu quả và tin cậy.

**Tài liệu tham khảo:**

- Richard S. Sutton, Andrew G. Barto (2018). *Reinforcement Learning: An Introduction (2nd edition)* – **Chương 4, 5** (Temporal-Difference Learning) và **Chương 6** (Value Function Approximation) cung cấp cái nhìn sâu về cách TD, Q-learning liên hệ với xấp xỉ ngẫu nhiên.
- Shiyu Zhao (2024). *Mathematical Foundations of Reinforcement Learning* – **Chương 6: Stochastic Approximation** trình bày nền tảng toán học của SA và **Chương 7** liên hệ với phương pháp Temporal-Difference trong RL.
- Herbert Robbins, Sutton Monro (1951). *A Stochastic Approximation Method*. **The Annals of Mathematical Statistics, 22(3)**: Bài báo gốc đề xuất thuật toán Robbins–Monro, thiết lập nền tảng cho SA.
- J. Kiefer, J. Wolfowitz (1952). *Stochastic Estimation of the Maximum of a Regression Function*. **Annals of Mathematical Statistics, 23(3)**: Bài báo gốc về thuật toán Kiefer–Wolfowitz.
- Vivek S. Borkar (2008). *Stochastic Approximation: A Dynamical Systems Viewpoint*. Cambridge Univ. Press: Sách chuyên khảo hiện đại về SA, bao gồm cả phân tích ODE method và các biến thể hai tầng.
- Spall, J. (1998). *An Overview of the Simultaneous Perturbation Method for Efficient Optimization*. Johns Hopkins APL Technical Digest: Tóm lược dễ hiểu về SPSA và các ứng dụng.
- Tsitsiklis, J. (1994). *Asynchronous Stochastic Approximation and Q-Learning*. **Machine Learning, 16(3)**: Phân tích hội tụ của Q-learning dưới góc nhìn SA bất đồng bộ.
- Polyak, B. & Juditsky, A. (1992). *Acceleration of Stochastic Approximation by Averaging*. **SIAM Journal on Control and Optimization, 30(4)**: Giới thiệu kỹ thuật trung bình Polyak giúp tăng tốc độ hội tụ trung bình.
- Dalal et al. (2018). *Finite Sample Analysis of Two-Time-Scale Stochastic Approximation with Applications to Reinforcement Learning*. **COLT 2018**: Kết quả phân tích rate hữu hạn cho SA hai tầng và áp dụng cho GTD, TDC trong RL.
- Devraj, A. & Meyn, S. (2019). *Zap Q-Learning*. **NeurIPS 2019**: Thuật toán Q-learning dạng Newton hai tầng giảm phương sai, với phân tích hội tụ.