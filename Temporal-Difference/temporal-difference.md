Tuyệt vời! Mình sẽ chuẩn bị một bài giảng chi tiết bằng tiếng Việt về các phương pháp Temporal-Difference (TD) trong học tăng cường, bao gồm giải thích lý thuyết, công thức toán học, ví dụ minh họa, và trích dẫn từ các tài liệu uy tín như *Reinforcement Learning: An Introduction* của Sutton & Barto và *Mathematical Foundations of Reinforcement Learning* của Shiyu Zhao. Mình sẽ gửi lại bài giảng cho bạn ngay khi hoàn thành.

# Các phương pháp Temporal-Difference (TD) trong Học tăng cường

## Khái niệm và trực giác về phương pháp TD

**Temporal-Difference (TD)** là một lớp phương pháp *học tăng cường* **mô hình-tự do** (model-free) đặc trưng bởi việc cập nhật giá trị dựa trên sự chênh lệch theo thời gian của những ước lượng liên tiếp. Nói cách khác, TD cho phép một *agent* (tác tử) **học từ trải nghiệm thực tế** như phương pháp Monte Carlo, nhưng đồng thời **bootstrapping** giống quy hoạch động – tức sử dụng ngay chính **ước lượng hiện tại** của hàm giá trị để cập nhật tiếp, thay vì chờ kết quả cuối cùng ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=Temporal%20difference%20,1)). Trực giác ở đây là: sau mỗi bước thời gian $t$, agent sẽ điều chỉnh dự đoán của giá trị trạng thái **ngay lập tức** dựa trên dự đoán tiếp theo ở thời điểm $t+1$, thay vì đợi đến khi kết thúc một *episode* (một ván trải nghiệm). Điều này có nghĩa là **phần sai khác giữa hai dự đoán liên tiếp theo thời gian** (temporal difference) được dùng làm tín hiệu để học.

Khác với phương pháp Monte Carlo chỉ cập nhật giá trị **sau khi** biết kết cục cuối cùng của *episode*, phương pháp TD cập nhật **từng bước một** trước khi kết cục được biết ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=While%20Monte%20Carlo%20methods%20only,illustrated%20with%20the%20following%20example)). Đây chính là ý tưởng *bootstrapping*: điều chỉnh dự đoán hiện tại tiến tới gần hơn với những dự đoán ở bước sau (vốn được xem là “chính xác hơn” về tương lai). Ví dụ trực quan: nếu ta muốn dự báo thời tiết thứ Bảy, phương pháp Monte Carlo tương tự việc chờ đến cuối tuần mới hiệu chỉnh dự báo (dựa trên kết quả thực tế). Còn TD thì khi tới thứ Sáu ta đã có thể ước lượng tốt hơn cho thứ Bảy và cập nhật **từ sớm** dự báo cho thứ Bảy (dù chưa thật sự đến thứ Bảy) ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=While%20Monte%20Carlo%20methods%20only,illustrated%20with%20the%20following%20example)). Nhờ cập nhật liên tục từng bước như vậy, phương pháp TD có thể học *online* (trong lúc tương tác) và đối phó được với các nhiệm vụ có tập tương tác rất dài hoặc vô hạn.

Vì sử dụng sự khác biệt tạm thời giữa các ước lượng liên tiếp, TD tạo ra một đại lượng gọi là **sai số TD** (*TD error*). Sai số TD đo lường chênh lệch giữa giá trị dự đoán hiện tại và **mục tiêu TD** – thường được xây dựng từ **phần thưởng nhận được cộng với giá trị ước lượng của trạng thái kế tiếp**. Chính sai số này là tín hiệu điều chỉnh: nếu sai số TD khác 0, agent biết rằng dự đoán ban đầu còn lệch và cần cập nhật. Trực giác: sai số TD = 0 nghĩa là “không có bất ngờ”, dự đoán hoàn toàn khớp với thực tế quan sát, còn sai số TD khác 0 nghĩa là dự đoán cần được điều chỉnh tăng hoặc giảm.

## So sánh phương pháp TD với Monte Carlo và Quy hoạch động

Phương pháp TD nằm ở giữa hai cực Monte Carlo (MC) và Quy hoạch động (Dynamic Programming - DP) trong phổ các phương pháp RL ([Temporal Difference Learning in Reinforcement Learning | by Shivam Mohan | Nerd For Tech | Medium](https://medium.com/nerd-for-tech/temporal-difference-learning-in-reinforcement-learning-cf13ed159fcb#:~:text=Temporal%20Difference%20Learning%20,of%20different%20Reinforcement%20Learning%20methods)) ([Temporal Difference Learning in Reinforcement Learning | by Shivam Mohan | Nerd For Tech | Medium](https://medium.com/nerd-for-tech/temporal-difference-learning-in-reinforcement-learning-cf13ed159fcb#:~:text=,not%20known%20instead%20the%20current%E2%80%A6)). Mỗi phương pháp có ưu và nhược điểm riêng:

- **Monte Carlo (MC):** Học giá trị *từ mẫu* trải nghiệm thực tế, **không cần mô hình** môi trường. MC cập nhật giá trị **sau mỗi episode hoàn thành**, dựa trên *return* (tổng phần thưởng) thực thu được. Do đợi đến cuối episode, MC **không bootstrapping** – mục tiêu cập nhật hoàn toàn dựa trên kết quả thực tế, nên về dài hạn ước lượng không bị sai lệch (*unbiased*). Tuy nhiên, MC **không phù hợp để cập nhật từng bước** (không *incremental*) ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=environment,The%20methods%20also%20differ%20in)). Phải đợi hết episode mới có dữ liệu cập nhật khiến MC không áp dụng được cho các nhiệm vụ liên tục/không có trạng thái kết thúc rõ ràng. Ngoài ra, do return mỗi episode có phương sai cao, MC thường **hội tụ chậm** (nhiễu lớn).

- **Quy hoạch động (DP):** Sử dụng **mô hình** xác suất chuyển trạng thái và phần thưởng để tính toán và cập nhật giá trị theo *phương trình Bellman*. DP là phương pháp **suy luận** chính xác (tính toán trị kỳ vọng qua mô hình), thường hội tụ nhanh nếu có mô hình đầy đủ. DP cũng cập nhật theo kiểu bootstrapping (giá trị trạng thái được cập nhật dựa trên giá trị các trạng thái kế cận). Tuy nhiên, nhược điểm lớn là DP **đòi hỏi mô hình đầy đủ** của môi trường (vd. phân phối xác suất chuyển và phần thưởng) ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=for%20solving%20finite%20Markov%20decision,also%20differ%20in%20several%20ways)), thứ thường không có sẵn trong các bài toán thực tế. Hơn nữa, DP thường tính toán *toàn cục* theo chu kỳ (sweep) qua mọi trạng thái, nên kém linh hoạt trong môi trường thực (khó cập nhật online theo trải nghiệm mẫu).

- **Temporal-Difference (TD):** Giống MC, phương pháp TD **không cần mô hình môi trường** (*model-free*). Giống DP, TD **cập nhật bootstrapping từng bước**, tức cập nhật *cấp thời gian thực* sau mỗi bước dựa trên giá trị ước lượng hiện tại của trạng thái kế tiếp ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=Temporal%20difference%20,1)). TD vì thế là phương pháp **hoàn toàn cập nhật từng bước (fully incremental)** – agent vừa tương tác môi trường vừa điều chỉnh giá trị liên tục ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=environment,The%20methods%20also%20differ%20in)). Điều này giúp TD áp dụng được cho các chuỗi tương tác dài vô tận (ví dụ chạy liên tục không có trạng thái kết thúc). Về lý thuyết, do dùng bootstrapping, TD chấp nhận một ít độ *bias* (thiên lệch) trong ước lượng so với giá trị thật. Tuy nhiên bù lại, TD thường có **phương sai thấp hơn** MC, dẫn đến tốc độ hội tụ nhanh hơn trong nhiều tình huống. Trên thực nghiệm, người ta thường thấy TD(0) **hội tụ nhanh hơn** so với MC với cùng hệ số học $\alpha$ trong các bài toán ngẫu nhiên ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=,over%20this%20number%20of%20episodes)). Chẳng hạn, trong bài toán *random walk* (chuỗi trạng thái ngẫu nhiên, xem ví dụ bên dưới), TD(0) học gần đúng giá trị thật hiệu quả hơn MC khi so sánh sai số bình phương trung bình theo số episode ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=,over%20this%20number%20of%20episodes)). Một cách nôm na, TD tận dụng được cả dữ liệu chưa hoàn chỉnh (dự đoán trung gian) để học, nên **hiệu quả dữ liệu** tốt hơn. Nhược điểm của TD là **phân tích lý thuyết phức tạp hơn** (do tính chất chuỗi Markov và bootstrap), nhưng các kết quả kinh điển (Sutton & Barto) đã chứng minh TD(0) hội tụ đến giá trị đúng $v_\pi$ dưới các điều kiện chuẩn về tham số học tập.

Tóm lại, TD kết hợp ưu điểm “**mẫu thực tiễn**” của MC với “**cập nhật gia tăng**” của DP ([Temporal Difference Learning in Reinforcement Learning | by Shivam Mohan | Nerd For Tech | Medium](https://medium.com/nerd-for-tech/temporal-difference-learning-in-reinforcement-learning-cf13ed159fcb#:~:text=,not%20known%20instead%20the%20current%E2%80%A6)). Trong nhiều tình huống, TD đạt hiệu quả cao hơn, nhưng MC và DP vẫn hữu ích trong những trường hợp đặc thù (ví dụ MC cho bài toán có tập kết thúc ngắn, hoặc DP khi có sẵn mô hình chính xác). Hiện nay, các thuật toán RL mạnh mẽ thường kết hợp cả ba hướng (ví dụ TD($\lambda$) kết hợp MC và TD, hay Dyna-Q kết hợp TD với mô hình học được) để tận dụng điểm mạnh của mỗi phương pháp.

## Thuật toán TD(0) – cập nhật giá trị theo sai số TD

**TD(0)** (TD-lambda với $\lambda=0$) là trường hợp đơn giản nhất của phương pháp TD, cập nhật giá trị trạng thái sau đúng **một bước quan sát** (*one-step TD*). Thuật toán TD(0) thường được dùng cho bài toán **dự đoán giá trị trạng thái** (policy evaluation) dưới một chính sách $\pi$ cố định. Mục tiêu là tìm gần đúng hàm giá trị $V^\pi(s)$ cho mọi trạng thái $s$. TD(0) khởi đầu với một giá trị ước lượng ban đầu $V(s)$ (có thể tùy ý, ví dụ bằng 0 hoặc ngẫu nhiên). Sau đó, agent **tương tác với môi trường theo chính sách $\pi$** và cập nhật $V$ sau mỗi bước chuyển, theo công thức tổng quát:

$
\quad V(S_t) \; \leftarrow \; V(S_t) + \alpha\,\big[\,R_{t+1} + \gamma\,V(S_{t+1}) \;-\; V(S_t)\big]\,,\
$

trong đó:
- $S_t$ và $S_{t+1}$ lần lượt là trạng thái tại thời điểm $t$ và $t+1$ (trạng thái hiện tại và trạng thái kế tiếp vừa quan sát được).
- $R_{t+1}$ là **phần thưởng nhận được** khi chuyển từ $S_t$ sang $S_{t+1}$. 
- $\alpha \in (0,1]$ là **tốc độ học (learning rate)** – hệ số điều chỉnh mức độ thay đổi, $\alpha$ càng lớn thì cập nhật càng “mạnh” (nhưng nếu quá lớn có thể gây dao động) ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=Image%3A%20%7B%5Cdisplaystyle%20V%28S_%7Bt%7D%29%5Cleftarrow%20%281,text%7BThe%20TD%20target)).
- $\gamma \in [0,1]$ là **hệ số chiết khấu (discount factor)** – xác định mức độ coi trọng phần thưởng tương lai so với hiện tại. $\gamma$ gần 1 nghĩa là phần thưởng tương lai được xem trọng (bài toán dài hạn), $\gamma$ thấp nghĩa là agent chỉ quan tâm phần thưởng trước mắt ([What is Temporal Difference (TD) learning in reinforcement learning?](https://milvus.io/ai-quick-reference/what-is-temporal-difference-td-learning-in-reinforcement-learning#:~:text=is%20navigating%20a%20grid,waiting%20for%20the%20episode%E2%80%99s%20outcome)).
- Biểu thức **$R_{t+1} + \gamma\,V(S_{t+1})$** được gọi là **mục tiêu TD** (*TD target*) – đây là giá trị mong đợi mới cho trạng thái $S_t$ dựa trên phần thưởng tức thời nhận được và **ước lượng hiện tại** về giá trị của trạng thái kế tiếp ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=where%20Image%3A%20,known%20as%20the%20TD%20error)). Có thể hiểu mục tiêu TD là dự đoán cải thiện về $V(S_t)$ sau khi thấy một bước tương lai.
- Sai khác **$\big[R_{t+1} + \gamma\,V(S_{t+1}) - V(S_t)\big]$** được gọi là **sai số TD** (*TD error*) – đo lường chênh lệch giữa giá trị dự đoán mới (mục tiêu TD) và giá trị cũ. Thuật toán điều chỉnh $V(S_t)$ tiến gần tới mục tiêu, với mức độ do $\alpha$ quyết định ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=where%20Image%3A%20,known%20as%20the%20TD%20error)). Nếu sai số TD = 0, $V(S_t)$ sẽ không đổi vì dự đoán đã khớp mục tiêu.

Công thức trên cho thấy $V(S_t)$ được cập nhật một phần theo hướng tới **trị “phần thưởng + giá trị kế tiếp”**. Lưu ý rằng nếu $S_{t+1}$ là trạng thái kết thúc (terminal), ta thường quy ước $V(\text{terminal}) = 0$. Khi đó mục tiêu TD chỉ còn $R_{t+1}$ (phần thưởng cuối cùng).

**Pseudocode thuật toán TD(0):** (đánh giá giá trị dưới chính sách $\pi$) ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=We%20then%20repeatedly%20evaluate%20the,10)):

1. **Khởi tạo** bảng giá trị $V(s)$ cho tất cả các trạng thái $s$ (ví dụ 0 hoặc giá trị tùy ý). Chọn $\alpha > 0$ (nhỏ, ví dụ 0.1) và $\gamma$ phù hợp.
2. **Lặp lại** cho mỗi *episode* (mỗi ván tương tác dưới chính sách $\pi$):  
   - Bắt đầu từ một trạng thái khởi đầu $S_0$.  
   - **Lặp mỗi bước** $t$ cho đến khi *episode* kết thúc:  
     1. Từ trạng thái $S_t$, chọn một hành động $A_t$ theo **chính sách $\pi$** hiện tại. (Trong bài toán đánh giá, $\pi$ được cho trước; ta có thể mô phỏng hành động của agent theo $\pi$).  
     2. Thực hiện hành động $A_t$ trong môi trường, nhận được **phần thưởng** $R_{t+1}$ và quan sát **trạng thái mới** $S_{t+1}$.  
     3. Cập nhật **giá trị** của trạng thái $S_t$ theo công thức TD(0):  
        $$V(S_t) \leftarrow V(S_t) + \alpha\,[\,R_{t+1} + \gamma\,V(S_{t+1}) - V(S_t)\,]\,.$$  
     4. Chuyển sang trạng thái kế: $S_{t} \gets S_{t+1}$.  
   - **Kết thúc tập** khi $S_{t+1}$ là trạng thái kết thúc. Lặp lại quá trình cho tập tiếp theo (nếu có).  

Sau nhiều lần lặp, $V(s)$ sẽ dần tiệm cận $V^\pi(s)$ (giá trị thực theo chính sách) nếu $\alpha$ được giảm dần hoặc đủ nhỏ theo một lịch thích hợp (điều này đảm bảo hội tụ về mặt lý thuyết).

**Phân tích:** Thuật toán TD(0) ở trên cho phép cập nhật giá trị **ngay trong quá trình agent đang tương tác**, thay vì đợi hoàn thành cả phiên tương tác. Mỗi bước cập nhật chỉ dùng thông tin cục bộ (phần thưởng tức thời và giá trị hiện có của trạng thái kế tiếp) nên rất linh hoạt. Về mặt kỳ vọng, dễ thấy $E[R_{t+1} + \gamma V(S_{t+1})] = E[G_t] = V^\pi(S_t)$ nếu $V$ đã khớp với giá trị thật ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=%7D%28S_)). Do đó sai số TD là *ước lượng không chệch* của sai số so với giá trị thật, và TD(0) có xu hướng giảm dần sai số này. Trong thực tế, TD(0) thường hội tụ nhanh và ổn định, đặc biệt trong các môi trường ngẫu nhiên nơi MC gặp khó khăn do phương sai cao ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=,over%20this%20number%20of%20episodes)).

*Chú ý:* TD(0) là nền tảng cho các phương pháp TD phức tạp hơn. Nếu mở rộng việc nhìn nhiều bước tương lai (TD($n$) hoặc kết hợp trung bình nhiều bước TD với hệ số $\lambda$), ta có thuật toán **TD($\lambda$)** tổng quát hơn. Khi $\lambda=1$, TD($1$) tương đương MC (vì sử dụng toàn bộ phần thưởng tương lai thực tế), còn $\lambda=0$ chính là TD(0) ở trên ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=The%20lambda%20%28Image%3A%20,13)). Các giá trị $\lambda$ trung gian cho phép đánh đổi giữa bias-variance của TD và MC.

## Thuật toán SARSA – On-Policy TD Control

Khi mục tiêu không chỉ đánh giá chính sách mà còn **tìm chính sách tối ưu**, ta cần kết hợp cập nhật TD với việc cải thiện chính sách. **SARSA** (State-Action-Reward-State-Action) là thuật toán TD cho bài toán *control* (điều khiển) **theo chính sách đang theo (on-policy)**. Tên SARSA xuất phát từ chuỗi trải nghiệm được sử dụng: trạng thái ($S_t$), hành động ($A_t$), phần thưởng ($R_{t+1}$), trạng thái mới ($S_{t+1}$), và hành động kế tiếp ($A_{t+1}$) – tất cả đều theo chính sách của agent.

Mục tiêu của SARSA là xấp xỉ hàm **giá trị hành động** $Q^\pi(s,a)$ cho chính sách hiện tại, đồng thời điều chỉnh chính sách dựa trên $Q$. Cuối cùng, với chính sách được cải thiện dần dần (thường $\varepsilon$-greedy dựa vào $Q$), $Q$ sẽ hội tụ tới $Q^*(s,a)$ – giá trị hành động tối ưu, và chính sách hội tụ tới chính sách tối ưu $\pi^*$. SARSA được gọi là *on-policy* vì **cập nhật $Q$ sử dụng hành động tiếp theo $A_{t+1}$ được chọn theo **chính sách hiện tại** (bao gồm cả sự khám phá). Do đó, SARSA “học những gì *agent* đang làm”. Nếu agent đang dùng một chính sách có $\varepsilon$-thăm dò (ví dụ $\varepsilon$-greedy), $Q$ sẽ đánh giá chính sách “$\varepsilon$-greedy” đó thay vì trực tiếp đánh giá chính sách tối ưu tuyệt đối.

**Công thức cập nhật SARSA (TD trên Q):**

$
\quad Q(S_t, A_t) \;\leftarrow\; Q(S_t, A_t) + \alpha\,\big[\,R_{t+1} + \gamma\,Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\big]\,. \
$

Đây chính là dạng mở rộng của công thức TD(0) cho **giá trị hành động**. So với công thức của TD(0) cho $V$, ta thay $V(S_{t+1})$ bằng $Q(S_{t+1}, A_{t+1})$ – giá trị của hành động kế tiếp theo chính sách tại trạng thái mới ([State–action–reward–state–action - Wikipedia](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action#:~:text=Image%3A%20%7B%5Cdisplaystyle%20Q%5E%7Bnew%7D%28S_%7Bt%7D%2CA_%7Bt%7D%29%5Cleftarrow%20%281,t%2B1)). Phần $R_{t+1} + \gamma\,Q(S_{t+1},A_{t+1})$ đóng vai trò **mục tiêu TD** cho Q, và sai số TD ở đây là $\delta = R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)$. Cập nhật SARSA sẽ điều chỉnh Q(s,a) tiến gần tới mục tiêu này với tỷ lệ $\alpha$ ([State–action–reward–state–action - Wikipedia](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action#:~:text=A%20SARSA%20agent%20interacts%20with,action%20observation)). Lưu ý nếu $S_{t+1}$ là trạng thái kết thúc, ta có thể đặt $Q(S_{t+1},A_{t+1}) = 0$.

**Thuật toán SARSA (on-policy control):**

1. **Khởi tạo** $Q(s,a)$ tùy ý cho mọi trạng thái $s$ và hành động $a$. (Ví dụ khởi tạo $Q=0$ hoặc giá trị ngẫu nhiên nhỏ. Nếu biết trước hướng tốt có thể khởi tạo “optimistic” để thúc đẩy khám phá).
2. **Lặp lại** cho mỗi *episode*:  
   - Khởi tạo trạng thái ban đầu $S_0$. Chọn hành động $A_0$ theo một **chính sách khám phá** dựa trên $Q$ hiện tại, ví dụ $\varepsilon$-greedy (chọn hành động tốt nhất với xác suất $1-\varepsilon$, và chọn ngẫu nhiên với xác suất $\varepsilon$ để đảm bảo khám phá).  
   - **Lặp mỗi bước** $t$ cho đến khi $S_t$ là trạng thái kết thúc:  
     1. Thực hiện hành động $A_t$ tại trạng thái $S_t$, quan sát **phần thưởng** $R_{t+1}$ và **trạng thái mới** $S_{t+1}$.  
     2. **Chọn hành động kế tiếp $A_{t+1}$ tại $S_{t+1}$ theo chính sách hiện tại** (ví dụ cũng $\varepsilon$-greedy dựa trên $Q$).  
     3. **Cập nhật** giá trị Q cho cặp $(S_t, A_t)$:  
        $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha\,[\,R_{t+1} + \gamma\,Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\,]\,. \quad$$  
     4. Chuyển trạng thái: $S_t \gets S_{t+1}$, $A_t \gets A_{t+1}$.  
   - **Kết thúc episode** khi $S_{t+1}$ terminal. Lặp lại cho episode tiếp theo.  

Trong quá trình học, $Q$ được sử dụng để chọn hành động (theo $\varepsilon$-greedy), và cũng được cập nhật từ chính các hành động đã chọn – do đó là on-policy. Nếu $\varepsilon$ giảm dần về 0, chính sách sẽ dần trở nên tham lam hơn theo $Q$, nhưng SARSA vẫn sẽ học giá trị tương ứng với mức $\varepsilon$ hiện tại. Kết quả là, khi $Q$ hội tụ, nó hội tụ đến $Q^{\pi}$ của **chính sách cuối cùng** mà agent thực thi. Nếu ta giảm $\varepsilon$ chậm theo thời gian (chiến lược GLIE – Greedy in the Limit with Infinite Exploration), chính sách cuối cùng sẽ là tham lam (greedy) tối ưu và $Q$ hội tụ về $Q^*$. 

**Đặc tính:** Vì SARSA cập nhật theo hành động *thực* đã chọn, nó tính đến cả những rủi ro do chính sách khám phá gây ra. Nó không “giả định” môi trường luôn theo chính sách tối ưu tương lai, thay vào đó học giá trị theo cách agent đang hành động. Điều này dẫn đến một hệ quả: SARSA thường an toàn hơn trong quá trình học. Một ví dụ kinh điển là bài toán *Cliff Walking*: agent phải đi từ điểm start đến goal dọc theo mép vực thẳm (cliff) – rơi xuống vực bị phạt nặng. SARSA trong quá trình học sẽ chọn đường đi an toàn hơn (tránh sát mép vực), vì nếu nó mạo hiểm theo chính sách $\varepsilon$-greedy và rơi, phần thưởng âm sẽ làm giảm giá trị các hành động mạo hiểm. Ngược lại, thuật toán off-policy như Q-learning (xem bên dưới) giả định theo chính sách tối ưu (bỏ qua việc agent có thể rơi do khám phá), nên thường đánh giá cao con đường ngắn nhất sát vực (vì theo chính sách tối ưu thì sẽ không rơi) ([State–action–reward–state–action - Wikipedia](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action#:~:text=Watkin%27s%20Q,while%20following%20an%20exploration%2Fexploitation%20policy)). Kết quả: SARSA học **“cẩn thận”** hơn, còn Q-learning học **“lạc quan”** hơn về giá trị tối ưu. Tuy nhiên cuối cùng, khi chính sách đã tối ưu và $\varepsilon \to 0$, cả hai đều có thể tìm được đường đi ngắn nhất (nếu có đủ khám phá).

## Thuật toán Q-learning – Off-Policy TD Control

**Q-learning** (Watkins, 1989) là một trong những thuật toán RL phổ biến nhất, thuộc nhóm TD *off-policy*. Cũng giống SARSA, Q-learning duy trì hàm hành động $Q(s,a)$ và hướng đến việc tìm $Q^*(s,a)$ cho mọi trạng thái $s$ và hành động $a$. Tuy nhiên, điểm khác biệt là **Q-learning không cập nhật theo hành động thực tế của chính sách hiện tại**, mà cập nhật hướng theo **hành động tối ưu có thể**. Nói cách khác, Q-learning học giá trị **theo một chính sách khác** – chính sách tham lam (greedy) với $Q$ hiện tại, **trong khi agent vẫn có thể hành động theo chính sách khám phá**. Vì lý do này, Q-learning được xếp vào loại *off-policy* (chính sách “học” khác với chính sách “hành động” của agent).

Công thức cập nhật Q-learning rất giống SARSA, chỉ khác ở **mục tiêu TD**: thay vì $Q(S_{t+1}, A_{t+1})$ (giá trị hành động *theo chính sách* tại trạng thái kế), Q-learning dùng **giá trị tối ưu ước lượng** tại trạng thái kế tiếp, tức $\max_{a'}Q(S_{t+1}, a')$. Cụ thể:

$
Q(S_t, A_t) \;\leftarrow\; Q(S_t, A_t) + \alpha\,\big[\,R_{t+1} + \gamma\,\max_{a'} Q(S_{t+1}, a') \;-\; Q(S_t, A_t)\big]\,. \
$

Ở đây, $a'$ chạy qua tất cả hành động có thể tại $S_{t+1}$; $\max_{a'}Q(S_{t+1}, a')$ đại diện cho **giá trị tốt nhất dự kiến** nếu ở trạng thái $S_{t+1}$ rồi hành động tối ưu từ đó về sau ([Q-Learning in Reinforcement Learning - GeeksforGeeks](https://www.geeksforgeeks.org/q-learning-in-python/#:~:text=3.%20Temporal%20Difference%20or%20TD)). Do đó, Q-learning cập nhật $Q(S_t, A_t)$ hướng tới **giá trị của hành động tốt nhất tiếp theo**, bất kể hành động đó có thực sự được chọn hay không. Nếu $S_{t+1}$ là terminal, ta lấy $\max_{a'}Q(S_{t+1}, a') = 0$. 

**Thuật toán Q-learning (off-policy control):**

1. **Khởi tạo** $Q(s,a)$ tùy ý cho mọi trạng thái và hành động. Chọn tham số $\alpha, \gamma$.
2. **Lặp lại** cho mỗi *episode*:  
   - Khởi tạo trạng thái $S_0$.  
   - **Lặp mỗi bước** cho đến khi $S_t$ kết thúc:  
     1. Chọn hành động $A_t$ từ $S_t$ theo một chính sách **khám phá** (ví dụ $\varepsilon$-greedy trên $Q$) – đây là chính sách *hành động* của agent để thu thập trải nghiệm.  
     2. Thực hiện $A_t$, nhận thưởng $R_{t+1}$ và trạng thái mới $S_{t+1}$.  
     3. **Cập nhật** $Q(S_t, A_t)$ theo công thức Q-learning:  
        $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha\,[\,R_{t+1} + \gamma \max_{a'}Q(S_{t+1}, a') - Q(S_t, A_t)\,]\,. $$  
     4. Chuyển trạng thái: $S_t \gets S_{t+1}$.  
   - **Kết thúc episode** khi $S_t$ terminal.

Agent vẫn cần một chính sách khám phá (như $\varepsilon$-greedy) để đảm bảo thăm được các trạng thái và hành động khác nhau. Tuy nhiên, **điểm mấu chốt** là **chính sách này không ảnh hưởng trực tiếp đến mục tiêu cập nhật** – mục tiêu luôn giả định chọn hành động tốt nhất tiếp theo. Về lâu dài, điều này có nghĩa Q-learning trực tiếp ước lượng $Q^*(s,a)$ (giá trị tối ưu) trong khi vẫn dùng trải nghiệm thu thập từ một chính sách khác (chính sách hành động với $\varepsilon$ để khám phá). Một cách hình tượng: Q-learning “giả vờ” rằng agent luôn làm tối ưu, và học theo giả định đó, mặc dù thỉnh thoảng agent có thể làm những hành động kém hơn để khám phá.

**Tính chất:** Nếu mỗi trạng thái-hành động được thăm vô hạn lần và $\alpha$ giảm dần phù hợp, Q-learning được chứng minh sẽ hội tụ về $Q^*$ với xác suất 1 (Watkins & Dayan). Trong thực tiễn, Q-learning thường hội tụ nhanh đến **chính sách tối ưu** hơn SARSA, bởi vì nó luôn đẩy giá trị về theo hướng lạc quan (giả sử sẽ làm tốt nhất). Tuy nhiên, trong quá trình học, Q-learning có thể gặp rủi ro do hành động khám phá gây ra (như ví dụ cliff ở trên) – bởi vì dù agent rơi xuống vực, Q-learning lần sau vẫn *sẽ* thử lại hành động mạo hiểm do nghĩ rằng nếu làm đúng (theo chính sách tối ưu) thì tốt. Kết quả là quá trình học có thể nhận nhiều phần thưởng âm hơn so với SARSA (SARSA thận trọng hơn) trước khi hội tụ. Dù vậy, khi đã hội tụ, **Q-learning tìm được chính sách tối ưu**. Thuật toán này rất phổ biến và là nền tảng cho nhiều mở rộng hiện đại, chẳng hạn **Deep Q Network (DQN)** dùng Q-learning kết hợp với mạng neural để giải quyết các bài toán với không gian trạng thái cực lớn.

## Ví dụ minh họa cụ thể

Để trực quan hóa cách hoạt động của các phương pháp TD, ta xét hai ví dụ: (1) Bài toán **chuỗi trạng thái ngẫu nhiên** minh họa TD(0) so với Monte Carlo trong dự đoán giá trị, và (2) Bài toán **mê cung đơn giản** minh họa cách SARSA/Q-learning cập nhật giá trị hành động sau mỗi bước.

**Ví dụ 1 (Dự đoán TD(0) trên chuỗi trạng thái):** Giả sử ta có một chuỗi trạng thái tuyến tính: **A – B – C – D – E**. Trạng thái A và E là *trạng thái kết thúc* (terminal). Khi episode kết thúc tại A, agent không nhận phần thưởng (0); khi kết thúc tại E, agent nhận phần thưởng +1. Mỗi bước, agent di chuyển ngẫu nhiên sang trái hoặc phải với xác suất bằng nhau. Giả sử chính sách $\pi$ ở đây là “đi ngẫu nhiên” như vậy. Nhiệm vụ là dự đoán **giá trị** $V^\pi(s)$ của mỗi trạng thái (xác suất cuối cùng sẽ đến trạng thái E và nhận thưởng). Rõ ràng, giá trị thực của các trạng thái sẽ tăng dần từ A tới E (A=0, E=1, và C (giữa) khoảng ~0.5).

- **Phương pháp Monte Carlo:** phải chờ đến khi episode kết thúc (chạm A hoặc E) mới biết được phần thưởng cuối cùng. Ví dụ, một episode có thể là: C → B → C → D → E (nhận +1 ở cuối). Đến cuối episode, MC mới dùng phần thưởng +1 để cập nhật giá trị cho **toàn bộ** các trạng thái đã thăm (C, B, C, D) hướng về +1. Ngược lại, nếu episode đi đến A (0 điểm), MC sẽ cập nhật tất cả trạng thái trong episode đó về hướng 0. Nếu ta chạy nhiều episode và trung bình, giá trị $V$ dần tiến đến giá trị thật. Tuy nhiên, mỗi lần cập nhật MC sử dụng cả một đoạn dài kết quả, nên giá trị dao động khá lớn qua từng episode (do phần thưởng cuối hoặc 0 hoặc 1). 

- **Phương pháp TD(0):** cập nhật liên bước nên phản hồi sớm hơn. Xét cùng episode C→B→C→D→E ở trên:
  - Ngay sau bước **C→B** (chưa biết kết cục cuối cùng), TD(0) đã **cập nhật $V(C)$** dựa trên quan sát phần thưởng 0 và giá trị hiện thời của $V(B)$. Nếu ban đầu $V$ của các trạng thái đều là 0.5, thì $V(C)$ sẽ được điều chỉnh một chút về phía $V(B)$. Trong trường hợp này $V(B)$ cũng ~0.5 nên $V(C)$ hầu như không đổi sau bước đầu.  
  - Sau bước **B→C**, TD lại cập nhật $V(B)$ dựa trên phần thưởng 0 và $V(C)$ mới (vừa được cập nhật chút ít).  
  - Tiếp tục, bước **C→D** cập nhật $V(C)$ hướng về $V(D)$.  
  - Bước **D→E** nhận được phần thưởng +1 và đến trạng thái kết thúc E, TD(0) sẽ **cập nhật $V(D)$** ngay: kéo $V(D)$ lên gần $1$ (vì $V(E)$=0 và $R=+1$, mục tiêu TD cho $D$ là 1). 
  - Episode kết thúc, $V(C)$ và $V(B)$ đã phần nào được nâng lên nhờ các bước trung gian (dù chưa phản ánh full +1). 
  - Ở episode sau, nếu lại ghé qua C hoặc D, những giá trị này sẽ tiếp tục được điều chỉnh dựa trên kết quả mới.

Kết quả, sau nhiều episode, cả TD và MC đều hội tụ $V(A)\to0, V(B)\to0.166..., V(C)\to0.5, V(D)\to0.833..., V(E)\to1$. Tuy nhiên, **TD(0) thường đạt gần giá trị đúng sớm hơn** MC ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=,over%20this%20number%20of%20episodes)). Trong những episode đầu tiên, MC có thể cập nhật rất cực đoan (1 hoặc 0) tùy kết cục, còn TD thì “miễn nhiễm” hơn do mỗi bước chỉ điều chỉnh một phần nhỏ dựa trên dự đoán hiện tại. Hình bên dưới (trích từ Sutton & Barto) minh họa sai số bình phương trung bình của TD(0) thấp hơn so với MC theo số lượng episode trong bài toán random walk 5 trạng thái này.

**Ví dụ 2 (SARSA/Q-learning trong bài toán mê cung):** Xét một **lưới 4x4 (maze)**, nơi góc trái dưới là trạng thái xuất phát và góc phải trên là mục tiêu. Mỗi bước đi (lên, xuống, trái, phải) đều không có phần thưởng (0) trừ khi agent đến mục tiêu sẽ nhận **+1** (episode kết thúc). Ta muốn tìm chính sách tối ưu để đi đến mục tiêu nhanh nhất.  

- Ban đầu, giá trị hành động $Q(s,a)$ có thể khởi tạo bằng 0 cho mọi ô và mọi hướng. Agent chưa biết gì nên sẽ khám phá ngẫu nhiên (ví dụ dùng $\varepsilon$-greedy với $\varepsilon$ khá cao). 
- Giả sử agent thử đi **Đông** từ ô xuất phát. Nó chuyển sang ô bên phải, nhận $R=0$ (chưa phải mục tiêu). **Q-learning** sẽ ngay lập tức **cập nhật** $Q(\text{Start}, \text{Đông})$ hướng tới $\gamma \max_{a'}Q(\text{Right}, a')$ (vì $R=0$). Hiện tại $Q$ của ô “bên phải” chưa có thông tin (đều ~0), nên $\max Q(\text{Right},\cdot)\approx0$. Do đó $Q(\text{Start}, \text{Đông})$ gần như không đổi sau bước đầu (sai số TD gần 0). 
- Từ ô bên phải, agent tiếp tục thử các hành động khác. Giả sử nó đi **Bắc**. Cứ như vậy cho đến khi lần đầu tiên agent chạm được ô mục tiêu (nhận $R=+1$). Giả sử đường đi đó là Start → Right → Right → Up → Goal. Ngay khi đến goal, Q-learning sẽ cập nhật mạnh giá trị của hành động **Up** ở ô trước goal, vì phần thưởng +1 làm mục tiêu TD = 1. Các hành động trước đó (Right, Right, ...) cũng đã được cập nhật dần dần sau mỗi bước dựa trên các ước lượng tương lai (bootstrapping). Kết thúc episode, các giá trị $Q$ trên đường vừa đi đều được nâng lên đáng kể. 
- Ở những lần chơi sau, xác suất agent chọn lại các hành động dẫn về goal (nhờ $Q$ cao hơn) sẽ tăng. Dần dần, **Q-learning học được chính xác giá trị của mỗi hành động** (ở trạng thái nhất định) là bao nhiêu phần trăm cơ hội đạt mục tiêu. Chính sách tham lam theo $Q$ sẽ là đường ngắn nhất đến goal. TD cập nhật sau **mỗi bước đi** giúp **lan truyền giá trị từ đích về nguồn**: ngay khi một ô lân cận goal được cập nhật giá trị cao, các ô xa hơn sẽ từng bước cập nhật theo (thông qua chuỗi sai số TD qua các bước). Agent không cần khám phá lại toàn bộ sau mỗi episode, nhờ đó học nhanh.

Trong ví dụ trên, nếu ta dùng **SARSA** thay vì Q-learning, quá trình tương tự nhưng cập nhật $Q$ sẽ theo **hành động thực tế** mà agent chọn tiếp theo. Nếu agent vẫn dùng $\varepsilon$-greedy, trong quá trình học có thể nó đôi lúc chọn hành động không tối ưu (do $\varepsilon$). SARSA sẽ cập nhật $Q$ phản ánh cả những bước “thăm dò” này. Điều này dẫn đến giá trị của những hành động rủi ro (như đi sát chướng ngại nguy hiểm) có thể thấp hơn so với Q-learning. Tuy nhiên, kết quả cuối cùng khi $\varepsilon$ giảm dần thì cả SARSA và Q-learning đều sẽ tìm được đường đi tốt. Sự khác biệt chủ yếu nằm ở quá trình: SARSA **thận trọng** hơn, còn Q-learning **táo bạo** hơn (vì luôn giả định kết cục tốt nhất từ trạng thái kế tiếp).

Những ví dụ trên cho thấy sức mạnh của phương pháp TD: **cập nhật liên tục từ trải nghiệm từng bước giúp lan truyền thông tin phần thưởng sớm và hiệu quả hơn**. Trong bài toán mê cung hay nhiều bài toán thực tế khác (robotics, game AI), TD-learning cho phép agent điều chỉnh chiến lược **sau mỗi hành động**, dần dần **“hiểu” giá trị của các trạng thái/hành động** nào dẫn đến mục tiêu ([What is Temporal Difference (TD) learning in reinforcement learning?](https://milvus.io/ai-quick-reference/what-is-temporal-difference-td-learning-in-reinforcement-learning#:~:text=TD%20learning%20is%20foundational%20to,robotics%20control%20or%20recommendation%20systems)). Nhờ tính *online* và *model-free*, các thuật toán TD như Q-learning và SARSA đã trở thành công cụ tiêu chuẩn trong học tăng cường hiện đại, ứng dụng từ điều khiển robot, chơi game, đến các hệ thống đề xuất. Các cải tiến sau này (như TD($\lambda$), chức năng mục tiêu kép, hay kết hợp mạng neuron) đều dựa trên nền tảng ý tưởng TD cơ bản: **học từ sai số giữa dự đoán hiện tại và dự đoán tiếp theo về tương lai** để cải thiện dần hiệu suất của agent. 

**Tài liệu tham khảo:** Phần trình bày được tổng hợp từ các nguồn kinh điển về RL, đặc biệt là sách *Reinforcement Learning: An Introduction* của Sutton & Barto ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=for%20solving%20finite%20Markov%20decision,also%20differ%20in%20several%20ways)) ([](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf#:~:text=,over%20this%20number%20of%20episodes)), cùng với một số tài liệu học thuật khác ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=Temporal%20difference%20,1)) ([State–action–reward–state–action - Wikipedia](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action#:~:text=A%20SARSA%20agent%20interacts%20with,action%20observation)). Độc giả quan tâm có thể tìm hiểu thêm về phân tích toán học của phương pháp TD trong giáo trình *Mathematical Foundations of Reinforcement Learning* (Shiyu Zhao, 2023), và các nghiên cứu về TD-learning (như sự kết hợp eligibility traces, TD(λ), hay ứng dụng TD trong neuroscience để mô hình hóa dopamine) ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=TD,12)) ([Temporal difference learning - Wikipedia](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=The%20TD%20algorithm%20%20has,or%20time%20step%20and%20the)). TD-learning là nền tảng vững chắc cho nhiều thuật toán học tăng cường, minh chứng cho sự hiệu quả của việc “học dần dần” từ những khác biệt theo thời gian.