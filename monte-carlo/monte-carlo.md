Tuyệt vời! Mình sẽ chuẩn bị một bài viết bằng tiếng Việt giải thích chi tiết về phương pháp Monte Carlo trong Reinforcement Learning, dựa trên các tài liệu gốc như *Reinforcement Learning: An Introduction* (Sutton & Barto), *Mathematical Foundations of Reinforcement Learning* (Shiyu Zhao), và các nguồn đáng tin cậy khác. Bài viết sẽ bao gồm mô tả lý thuyết, các giả định, cách hoạt động, và ví dụ minh hoạ để giúp bạn hiểu rõ cách áp dụng phương pháp này.

Mình sẽ gửi lại cho bạn sớm nhé!

# Phương pháp Monte Carlo trong Reinforcement Learning (Học tăng cường)

## Giới thiệu khái quát về Reinforcement Learning và vai trò của phương pháp Monte Carlo

**Reinforcement Learning (Học tăng cường)** là lĩnh vực học máy trong đó một *tác tử (agent)* học cách hành động trong một *môi trường (environment)* bằng cách tương tác thử nghiệm và nhận *phần thưởng (reward)* phản hồi. Mục tiêu của tác tử là tối ưu hóa tổng phần thưởng nhận được theo thời gian. Bài toán RL thường được mô hình hóa dưới dạng **Markov Decision Process (MDP)** gồm tập trạng thái, tập hành động, hàm phần thưởng và (nếu biết) quy luật chuyển trạng thái. Khác với học có giám sát, tác tử RL không được cung cấp sẵn đầu ra đúng cho mỗi đầu vào mà phải tự khám phá chiến lược thông qua **trải nghiệm (experience)**.

Trong RL, có hai cách tiếp cận chính để tìm chiến lược tối ưu: **phương pháp có mô hình (model-based)** và **phương pháp không mô hình (model-free)**. Phương pháp **Dynamic Programming (Quy hoạch động)** là kiểu model-based truyền thống: tác tử biết trước động lực môi trường (xác suất chuyển trạng thái, hàm thưởng) và sử dụng các công thức Bellman để tính toán giá trị và cải thiện chính sách. Ngược lại, phương pháp **Monte Carlo (MC)** là một trong những kỹ thuật model-free cơ bản: tác tử **không cần kiến thức về mô hình môi trường**, thay vào đó học trực tiếp từ những tập trải nghiệm thu được khi tương tác ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=First%20learning%20method%20for%20estimating,we%20learn%20only%20by%20experience)) ([](https://page-one.springer.com/pdf/preview/10.1007/978-981-97-3944-8_5#:~:text=based%20on%20the%20system%20model,If%20we%20do%20not%20have)). Monte Carlo tận dụng việc **lấy mẫu ngẫu nhiên** thông qua các *tập* (episodes) tương tác để ước lượng giá trị kỳ vọng của các trạng thái hoặc hành động. Nói cách khác, thay vì tính toán giá trị trạng thái dựa trên mô hình toán học, MC **ước lượng giá trị bằng cách trung bình hóa các phần thưởng nhận được** qua nhiều lần thử ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=Monte%20Carlo%20methods%20are%20based,step%20sense)). Phương pháp Monte Carlo giữ vai trò quan trọng trong RL: nó là kỹ thuật đầu tiên giúp *ước lượng hàm giá trị* và *tìm kiếm chính sách tối ưu* chỉ từ dữ liệu trải nghiệm, mở đường cho các thuật toán học tăng cường hiện đại mà không đòi hỏi hiểu biết tường minh về môi trường ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=First%20learning%20method%20for%20estimating,we%20learn%20only%20by%20experience)). 

Tổng quan, Monte Carlo trong RL dùng để **dự đoán giá trị (value prediction)** cho một chính sách nhất định và **điều khiển (control)** để cải thiện chính sách, thông qua việc chạy mô phỏng nhiều lần. Phần sau sẽ đi sâu vào nguyên lý hoạt động của phương pháp này, các biến thể on-policy/off-policy, so sánh với Dynamic Programming, điều kiện áp dụng, hạn chế và một ví dụ minh họa chi tiết.

## Lý thuyết của phương pháp Monte Carlo: Nguyên lý học từ trải nghiệm và cập nhật giá trị

Phương pháp Monte Carlo trong RL học thông qua **trải nghiệm đầy đủ các tập tương tác**. Mỗi tập bắt đầu từ một trạng thái xuất phát và tiếp diễn cho đến khi kết thúc (một trạng thái kết thúc hoặc sau một số bước cố định). Trong quá trình đó, tác tử theo một chính sách $\pi$ nhất định, tạo ra một chuỗi trạng thái, hành động và phần thưởng: $S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T$ (trong đó $S_T$ là trạng thái kết thúc của tập và không có hành động tiếp theo). **Tổng phần thưởng tích lũy** từ thời điểm $t$ (gọi là *return* $G_t$) được định nghĩa là: 

$$G_t = R_{t+1} + R_{t+2} + \dots + R_T,$$ 

(có thể bao gồm hệ số chiết khấu nếu xét bài toán dài hạn, nhưng ở đây giả sử phần thưởng không chiết khấu để đơn giản). Mục tiêu của MC là ước lượng **hàm giá trị** $V^{\pi}(s)$ = kỳ vọng của $G_t$ khi bắt đầu từ trạng thái $s$ và theo chính sách $\pi$. Do không biết trước kỳ vọng này, ta **lấy mẫu nhiều lần** và sử dụng **trung bình mẫu** để xấp xỉ nó ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=Monte%20Carlo%20methods%20are%20based,step%20sense)).

**Nguyên lý hoạt động:** Monte Carlo lưu lại các kết quả nhận được sau mỗi tập và dần dần cập nhật giá trị trạng thái dựa trên trung bình các phần thưởng thu được. Cụ thể, giả sử ta muốn ước lượng $v_{\pi}(s)$ cho một trạng thái $s$ dưới chính sách $\pi$. Ta cho tác tử **chơi nhiều tập** theo $\pi$. Mỗi khi trạng thái $s$ xuất hiện trong một tập, ta ghi nhận *return* thực tế $G$ thu được kể từ lần xuất hiện đó cho đến cuối tập. Sau nhiều tập, ta sẽ có một tập hợp các giá trị $G$ thu được khi bắt đầu từ $s$. **Giá trị của trạng thái $s$** được ước lượng bằng **trung bình các giá trị $G$ này**: 

- $V(s) \approx \dfrac{1}{N} \sum_{i=1}^{N} G^{(i)}$, 

với $G^{(i)}$ là tổng thưởng thu được từ trạng thái $s$ trong tập thứ $i$ mà $s$ xuất hiện. Khi $N$ tiến đến vô hạn (tức số tập rất lớn), luật số lớn chỉ ra rằng giá trị trung bình mẫu sẽ hội tụ về đúng kỳ vọng $v_{\pi}(s)$ ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=An%20obvious%20way%20to%20estimate,converge%20to%20the%20expected%20value)).

**First-visit MC và Every-visit MC:** Có hai cách cập nhật phổ biến: *first-visit* và *every-visit*. *First-visit MC* cập nhật giá trị trạng thái $s$ **chỉ dùng lần đầu tiên $s$ được ghé thăm trong mỗi tập** (bỏ qua các lần ghé thăm sau trong cùng tập), trong khi *Every-visit MC* cập nhật dựa trên **tất cả các lần** $s$ xuất hiện (trung bình tất cả returns sau mọi lần ghé thăm) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=,after%20each%20visit%20to%20%24s)). Cả hai cách tiếp cận đều đảm bảo hội tụ về giá trị thật khi số tập tăng lên đủ lớn, nhưng first-visit MC thường được dùng nhiều vì đơn giản và mỗi tập đóng góp một giá trị cho mỗi trạng thái thay vì nhiều lần.

**Cập nhật giá trị và học từ trải nghiệm:** Thuật toán MC cơ bản cho dự đoán giá trị sẽ như sau ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=%21%20Idea%3A%20Average%20returns%20observed,episode%20%21%20Both%20converge%20asymptotically)) ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=%21%20First,episode%20%21%20Both%20converge%20asymptotically)): 

1. Khởi tạo $V(s)$ ngẫu nhiên cho mọi trạng thái $s$.  
2. Thực hiện nhiều tập tương tác dưới chính sách $\pi$.  
3. Với mỗi trạng thái $s$ xuất hiện trong tập, tính *return* $G$ kể từ lần xuất hiện (đầu tiên) đó. Thêm $G$ vào danh sách các returns của trạng thái $s$.  
4. Cập nhật $V(s)$ = trung bình của tất cả các returns thu thập được trong danh sách. (Hoặc cập nhật tăng dần $V(s) \leftarrow V(s) + \alpha [G - V(s)]$ với $\alpha$ là tốc độ học, tương đương với tính trung bình cộng dồn).  
5. Lặp lại đến khi $V(s)$ hội tụ (không đổi đáng kể giữa các lần cập nhật).

Điểm mấu chốt là MC **sử dụng toàn bộ phần thưởng của cả tập** để cập nhật giá trị của các trạng thái trong tập đó. Không giống như Dynamic Programming hay Temporal-Difference (TD) sử dụng phương trình Bellman để cập nhật từng bước dựa trên ước lượng hiện tại, Monte Carlo **không “bootstrapping” từ các giá trị ước lượng hiện có mà dùng trực tiếp kết quả thực tế cuối cùng** ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=Blackjack%20value%20functions)) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=We%20can%20apply%20backup%20diagrams,instead%20of%20all%20possible%20transitions)). Mỗi trạng thái được cập nhật hoàn toàn độc lập dựa trên các trải nghiệm chứa nó, không phụ thuộc vào giá trị hiện tại của các trạng thái kế tiếp. Chính vì vậy, MC thuộc loại **phương pháp không bootstrapping**, và được xem là *unbiased* (không chệch) vì sử dụng giá trị thật (dựa trên mẫu) chứ không dùng ước lượng trung gian. Tuy nhiên, do dựa trên mẫu ngẫu nhiên, ước lượng MC có thể có **phương sai lớn** – cần rất nhiều tập để giá trị trung bình hội tụ ổn định.

Một ưu điểm quan trọng của MC là **dễ hiểu và dễ triển khai**: chỉ cần chạy mô phỏng nhiều lần và trung bình kết quả. Ngoài ra, **thời gian tính toán giá trị cho một trạng thái không phụ thuộc vào số lượng trạng thái toàn cục** (vì ta có thể chỉ tập trung mô phỏng những tập liên quan trạng thái đó) ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=Blackjack%20value%20functions)). Điều này khác với DP khi muốn cập nhật một trạng thái phải xem xét tất cả khả năng chuyển tiếp đến các trạng thái khác theo mô hình. Tuy nhiên, nhược điểm lớn của MC là nó **đòi hỏi các tập phải kết thúc** (episodic) để thu thập phần thưởng cuối cùng $G_T$. Ta **phải chờ đến cuối mỗi tập** mới biết được tổng thưởng để cập nhật, nên không thể cập nhật giá trị giữa chừng trong một tập đang diễn ra ([Foundations of Reinforcement Learning  Model-free RL: Monte Carlo and temporal difference (TD) learning](https://users.ece.cmu.edu/~yuejiec/ece18813B_notes/lecture8-model-free-MC-TD.pdf#:~:text=Caveat%20of%20Monte%20Carlo%20methods%3A,11)). Điều này khiến MC không áp dụng trực tiếp được cho các tác vụ không có trạng thái kết thúc (tiếp diễn vô hạn) nếu không có cách chia tập nhân tạo. Ngoài ra, số tập cần thiết có thể rất lớn, đặc biệt khi không phải mọi trạng thái đều được ghé thăm thường xuyên – MC yêu cầu chính sách phải **thăm dò đủ** (sẽ nói thêm ở phần sau) để mọi trạng thái/hành động đều có cơ hội nhận thưởng và được cập nhật giá trị.

Tóm lại, lý thuyết Monte Carlo trong RL dựa trên việc *học qua trải nghiệm*: thu thập nhiều kết quả từ trạng thái/hành động và dùng **trung bình mẫu** để xấp xỉ giá trị kỳ vọng. Nhờ đó, tác tử có thể dần dần xây dựng chính xác hàm giá trị cho chính sách hiện tại chỉ thông qua tương tác thử sai lặp đi lặp lại.

## Monte Carlo Prediction và Monte Carlo Control (On-policy vs Off-policy)

Phương pháp Monte Carlo được ứng dụng vào hai nhiệm vụ chính trong RL: **Prediction (dự đoán giá trị)** và **Control (tìm kiếm/chọn lựa chính sách tối ưu)**. Nói một cách đơn giản, *prediction* là đánh giá “chất lượng” của một chính sách đã cho, còn *control* là cải thiện chính sách dựa trên các đánh giá đó. Chúng ta sẽ xem cách MC giải quyết từng nhiệm vụ, và phân biệt hai cách tiếp cận: **on-policy** và **off-policy**.

### Monte Carlo Prediction (Ước lượng giá trị của chính sách cho trước)

Trong bài toán dự đoán, ta giả sử có một chính sách $\pi$ cố định (ví dụ: một bảng xác suất chọn hành động ở mỗi trạng thái). Nhiệm vụ là ước lượng hàm giá trị $V^{\pi}(s)$ (hoặc $Q^{\pi}(s,a)$ nếu xét giá trị hành động) cho *mọi trạng thái* $s$ dưới chính sách đó. **Monte Carlo prediction** thực hiện điều này bằng cách chạy nhiều tập theo chính sách $\pi$ và áp dụng nguyên lý như mô tả ở phần trên: *giá trị của trạng thái là trung bình các phần thưởng thu được sau những lần trạng thái đó xuất hiện* ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=An%20obvious%20way%20to%20estimate,converge%20to%20the%20expected%20value)). MC prediction không yêu cầu mô hình môi trường, chỉ cần khả năng sinh ra trải nghiệm theo $\pi$ (tức là mô phỏng hoặc tương tác thật với môi trường theo chính sách $\pi$).

Để đảm bảo mỗi trạng thái đều được thăm dò đủ, thường ta cần $\pi$ là một **chính sách khám phá**: nghĩa là mọi hành động từ mọi trạng thái đều có xác suất > 0 được thực hiện. Nếu chính sách $\pi$ mang tính quyết định cứng nhắc (luôn chọn một hành động duy nhất ở một trạng thái), có thể một số trạng thái/hành động sẽ không bao giờ được trải nghiệm, khiến giá trị của chúng không thể ước lượng chính xác. Một kỹ thuật thường dùng để đảm bảo thăm dò trong prediction là **Exploring Starts** – khi sinh tập mới, ta bắt đầu từ một trạng thái và hành động ngẫu nhiên, qua đó về lâu dài mọi trường hợp đều sẽ xuất hiện ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=Problem%3A%20many%20state,We%20must%20assure%20continual%20exploration)). Ngoài ra, nếu $\pi$ ban đầu không đủ ngẫu nhiên, ta có thể tạm dùng một chính sách khám phá (ví dụ $\epsilon$-greedy) để thu thập dữ liệu cho việc đánh giá $\pi$ – khi đó ta đang làm *off-policy prediction* (sẽ nói sau). Nhưng trong bối cảnh prediction thuần túy, ta thường giả sử $\pi$ đã bao quát các hành động có thể.

Kết quả của Monte Carlo prediction là một bảng giá trị $V(s)$ hoặc $Q(s,a)$ xấp xỉ cho chính sách $\pi$. Trong sách của Sutton & Barto, ví dụ *đánh giá chính sách chơi Blackjack* bằng MC (trình bày ở phần ví dụ) cho thấy phương pháp này hiệu quả như thế nào trong việc ước lượng giá trị kỳ vọng của trạng thái mà không cần bất kỳ kiến thức nào về luật xác suất trong game ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=Example%3A%20Blackjack%20%E2%80%A2%20Goal%3A%20Achieve,20%20or%2021%2C%20else%20hit)).

### Monte Carlo Control (Điều khiển – tìm chính sách tối ưu)

Bài toán *control* khó hơn: thay vì đánh giá một chính sách cố định, ta muốn **tìm ra chính sách tối ưu** $\pi^*$ mang lại phần thưởng trung bình cao nhất từ mọi trạng thái. Monte Carlo giải quyết bài toán này bằng cách kết hợp đánh giá (prediction) và cải thiện (improvement) chính sách theo nguyên lý của **Generalized Policy Iteration (GPI)**: lặp đi lặp lại việc ước lượng giá trị dưới chính sách hiện tại và sau đó **cập nhật chính sách tham lam (greedy) theo giá trị đó** ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=This%20section%20is%20about%20how,policy%20and%20optimal%20value%20function)) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=Policy%20improvement%20is%20done%20by,value%20function)). Cụ thể, quy trình như sau:

- Khởi tạo một chính sách $\pi_0$ (có thể ngẫu nhiên hoặc tùy ý nhưng phải bảo đảm mọi hành động đều có cơ hội – chi tiết bên dưới).  
- **Chính sách đánh giá (Policy Evaluation):** Sử dụng Monte Carlo prediction để ước lượng hàm giá trị *hành động* $Q^{\pi_k}(s,a)$ cho chính sách hiện tại $\pi_k$. (Lưu ý: trong control, ta thường tập trung ước lượng **giá trị hành động $Q(s,a)$** thay vì chỉ $V(s)$, vì nếu không có mô hình, việc chọn hành động tốt nhất dựa trên $V(s)$ là không trực tiếp – ta cần biết hành động nào dẫn đến trạng thái nào. Với $Q(s,a)$, ta chỉ việc chọn hành động có $Q$ cao nhất tại mỗi trạng thái ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=If%20the%20model%20of%20a,p%24%20here)) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=If%20the%20model%20of%20a,p%24%20here))).  
- **Cải thiện chính sách (Policy Improvement):** Từ hàm $Q^{\pi_k}$ vừa ước lượng, tạo chính sách mới $\pi_{k+1}$ bằng cách **tham lam với $Q$**: tại mỗi trạng thái $s$, $\pi_{k+1}$ chọn hành động $a^* = \arg\max_a Q^{\pi_k}(s,a)$. $\pi_{k+1}$ chính là chính sách tốt hơn hoặc bằng $\pi_k$ theo Định lý cải thiện chính sách.  
- Lặp lại: Đặt $k := k+1$ và lặp lại chu kỳ *đánh giá bằng MC* (với các tập mới theo $\pi_k$) rồi *cải thiện*. Quá trình này tiếp diễn cho đến khi chính sách không thay đổi nữa (đạt tối ưu).

Vì Monte Carlo đánh giá chính sách dựa trên mẫu, về mặt lý thuyết ta cần số lượng tập vô hạn ở mỗi bước đánh giá để đạt giá trị chính xác tuyệt đối. Tuy nhiên, tương tự như policy iteration thông thường, trong thực tế ta có thể xen kẽ một cách *không tuần tự tuyệt đối*: ví dụ **đánh giá xong mỗi tập là cải thiện luôn** một lượt (MC thường làm kiểu này: sau mỗi tập, cập nhật $Q$ cho các trạng thái-hành động đã thấy và cải thiện chính sách ngay trên những trạng thái đó ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=,1%20in%20value%20iteration))). Điều này tương đương với thực hiện *policy iteration không đồng bộ*, và về lâu dài vẫn hướng tới tối ưu.

**On-policy vs Off-policy trong Monte Carlo Control:** Một thách thức trong MC control là đảm bảo **khám phá đủ**. Nếu ta luôn chọn hành động tham lam theo $Q$ hiện tại, có nguy cơ tác tử bỏ qua những hành động có thể có giá trị cao chưa được thử đến – dẫn đến kết quả chính sách có thể bị kẹt ở mức cục bộ. Để tránh điều này, có hai hướng tiếp cận:

- **On-policy MC control:** Ta sẽ *học và cải thiện **ngay trên chính sách đang được thi hành***. Điều này đòi hỏi chính sách luôn phải có tính ngẫu nhiên nhất định để khám phá. Cách làm phổ biến là dùng **chính sách mềm (soft policy)**, ví dụ **$\boldsymbol{\epsilon}$-greedy**: hầu hết thời gian chọn hành động tham lam, nhưng thỉnh thoảng (với xác suất $\epsilon$) chọn ngẫu nhiên một hành động khác ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=Blackjack%20example%20continued%20%21%20Exploring,Initial%20policy%20as%20described%20before)). Nhờ đó, mọi hành động đều có xác suất được chọn khác 0 tại mỗi trạng thái. Thuật toán cụ thể gọi là **On-policy First-Visit MC Control với $\epsilon$-soft**:  
  1. Khởi tạo $Q(s,a)$ tùy ý và chính sách $\pi$ là $\epsilon$-soft (ví dụ $\epsilon$-greedy).  
  2. Mỗi tập: thực thi theo $\pi$ hiện tại, thu thập phần thưởng và cập nhật $Q(s,a)$ cho mọi cặp trạng thái-hành động xuất hiện (theo first-visit MC, trung bình hóa các returns).  
  3. Sau tập đó, cải thiện chính sách $\pi$: tại mỗi trạng thái vừa ghé thăm, chọn hành động có $Q$ cao nhất làm hành động tham lam, các hành động khác vẫn giữ một xác suất nhỏ $\epsilon$ (đảm bảo $\pi$ luôn $\epsilon$-soft) ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=Blackjack%20example%20continued%20%21%20Exploring,Initial%20policy%20as%20described%20before)).  
  4. Lặp lại.  

  Với phương pháp on-policy, chính sách *vừa học vừa thực thi* này sẽ dần dần hội tụ tới **chính sách tối ưu $\epsilon$-soft** (và khi $\epsilon$ rất nhỏ, chính sách gần tối ưu thuần túy). Sutton & Barto chứng minh rằng nếu mọi cặp $(s,a)$ tiếp tục được thăm dò, quá trình này sẽ không dừng ở một chính sách kém tối ưu nào – cuối cùng $Q$ và $\pi$ sẽ ổn định khi cả hai đạt tối ưu (vì nếu $\pi$ chưa tối ưu, $Q$ ước lượng sẽ khiến $\pi$ đổi lựa chọn) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=Image%20Fig%205,Exploring%20starts)).

- **Off-policy MC control:** Thay vì sử dụng cùng một chính sách để vừa thu thập dữ liệu vừa cải thiện, off-policy tách biệt hai chính sách: **chính sách hành vi (behavior policy)** dùng để sinh trải nghiệm, và **chính sách đích (target policy)** là cái ta thực sự muốn tối ưu hóa ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=%2A%20on,used%20to%20generate%20the%20data)). Trong ngữ cảnh này, chính sách hành vi thường được chọn sao cho **mang tính khám phá cao** (ví dụ chọn hành động ngẫu nhiên hoặc $\epsilon$-greedy rộng rãi), đảm bảo mọi hành động đều có cơ hội. Chính sách đích thường là *tham lam* (deterministic) hướng tới tối ưu. Chúng ta sẽ sử dụng dữ liệu do behavior policy sinh ra để ước lượng giá trị cho target policy. Tuy nhiên, do hai chính sách khác nhau, phân phối trạng thái-hành động trong tập mẫu không khớp với phân phối theo target policy, cần một cơ chế hiệu chỉnh. **Importance Sampling (Lấy mẫu quan trọng)** được sử dụng cho mục đích này: mỗi *return* từ tập thu thập được sẽ được **gán trọng số $W$** bằng **tỷ số xác suất** rằng tập đó xảy ra dưới target policy so với dưới behavior policy ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=Off,probabilities%20in%20the%20estimation%20policy)). Ví dụ, nếu một chuỗi hành động $(a_0, a_1, ..., a_{T-1})$ xảy ra với xác suất $P$ theo behavior và xác suất $P^*$ theo target, thì trọng số $\rho = P^*/P$ sẽ được nhân vào phần thưởng của chuỗi đó khi cập nhật cho target. Thực hành phổ biến trong off-policy MC là dùng **weighted importance sampling** (lấy mẫu quan trọng có trọng số) để giữ cho trọng số luôn được chuẩn hóa (tổng trọng số = 1), giúp giảm phương sai so với công thức importance sampling thông thường ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=,and%20thus%20is%20strongly%20preferred)). Kết quả là ta thu được ước lượng $Q^{\text{target}}(s,a)$ mặc dù các tập dữ liệu được sinh bởi behavior policy.

  Off-policy MC cho phép, chẳng hạn, **học chính sách tối ưu trong khi vẫn áp dụng một chính sách khám phá khác**. Một ứng dụng thực tế là trong môi trường thực tế (như huấn luyện robot), ta muốn thu thập dữ liệu an toàn (behavior policy thiên về khám phá an toàn), đồng thời học về một chính sách tối ưu hơn (target policy) mà không gây rủi ro trong quá trình học. Tuy nhiên, nhược điểm lớn của off-policy MC là trọng số importance sampling có thể dao động rất lớn, dẫn đến **phương sai cao hoặc thậm chí vô hạn** trong ước lượng ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=,and%20thus%20is%20strongly%20preferred)) nếu chính sách đích quá khác biệt và hiếm khi “giả lập” được bởi chính sách hành vi. Để giảm thiểu, người ta ưu tiên weighted sampling và cũng thường chọn behavior policy đủ “phủ” target policy (ví dụ behavior là $\epsilon$-soft và giảm dần $\epsilon$ theo thời gian để dần tiệm cận target).

Tóm lại, **on-policy MC control** học chính sách tối ưu bằng cách liên tục điều chỉnh chính sách đang thực thi (duy trì tính ngẫu nhiên để khám phá), trong khi **off-policy MC control** học chính sách tối ưu gián tiếp từ dữ liệu của một chính sách khác. Cả hai đều hướng đến giải quyết bài toán điều khiển, nhưng on-policy đơn giản hơn (không cần importance sampling) và được dùng nhiều trong thực nghiệm, còn off-policy tổng quát hơn về mặt lý thuyết (cho phép đánh giá bất kỳ chính sách nào từ dữ liệu khác). 

## So sánh Monte Carlo với Dynamic Programming (và các phương pháp khác)

Monte Carlo có những điểm tương đồng và khác biệt quan trọng với phương pháp Dynamic Programming (DP) – vốn cũng nhằm mục tiêu tính giá trị và tìm chính sách tối ưu nhưng yêu cầu mô hình môi trường. Dưới đây là bảng so sánh giữa MC và DP:

| Khía cạnh               | **Monte Carlo (MC)**                                         | **Dynamic Programming (DP)**                                  |
|-------------------------|--------------------------------------------------------------|--------------------------------------------------------------|
| **Kiến thức môi trường**| *Model-free:* **Không cần** mô hình xác suất chuyển tiếp hay phần thưởng. Học trực tiếp từ dữ liệu tương tác ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=First%20learning%20method%20for%20estimating,we%20learn%20only%20by%20experience)). | *Model-based:* **Cần có** mô hình đầy đủ của MDP (xác suất chuyển trạng thái và phần thưởng) để tính toán giá trị. |
| **Nguyên lý cập nhật**   | **Lấy mẫu và trung bình:** Sử dụng *returns* từ các tập thử nghiệm thực tế để cập nhật giá trị. **Không bootstrap** – dùng giá trị phần thưởng thực thu được đến cuối tập ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=We%20can%20apply%20backup%20diagrams,instead%20of%20all%20possible%20transitions)). | **Bootstrapping theo Bellman:** Sử dụng công thức Bellman với mô hình để cập nhật giá trị dựa trên giá trị của các trạng thái kế tiếp (tính toán dạng kỳ vọng hoặc tối ưu một bước). |
| **Yêu cầu tập (episode)**| **Cần tập kết thúc:** Chỉ áp dụng trực tiếp cho bài toán phân đoạn (episodic). Phải đợi tập kết thúc mới cập nhật được giá trị ([Foundations of Reinforcement Learning  Model-free RL: Monte Carlo and temporal difference (TD) learning](https://users.ece.cmu.edu/~yuejiec/ece18813B_notes/lecture8-model-free-MC-TD.pdf#:~:text=Caveat%20of%20Monte%20Carlo%20methods%3A,11)). Không hoạt động tốt nếu nhiệm vụ không có trạng thái kết thúc (trừ khi cắt thành tập nhân tạo). | **Không cần tập kết thúc:** Có thể áp dụng cho cả bài toán tiếp diễn (infinite-horizon) bằng cách giải phương trình Bellman với hệ số chiết khấu. DP không phụ thuộc vào khái niệm tập trải nghiệm. |
| **Hiệu quả tính toán**   | **Mẫu độc lập:** Thời gian ước lượng một trạng thái chủ yếu phụ thuộc số lần trạng thái đó được ghé thăm trong mẫu, **không phụ thuộc kích thước không gian trạng thái tổng** ([Chapter 5.ppt](https://people.cs.umass.edu/barto/courses/cs687/Chapter%205.pdf#:~:text=Blackjack%20value%20functions)). Thích hợp khi mô phỏng nhanh hoặc số trạng thái lớn (vì không cần duyệt qua toàn bộ trạng thái). | **Toàn cục:** Phải tính toán trên *toàn bộ không gian trạng thái* (ví dụ, cập nhật giá trị từng trạng thái trong mỗi bước lặp). Với không gian trạng thái lớn, DP có thể chậm hoặc không khả thi do chi phí tính toán tăng mạnh theo số trạng thái. |
| **Độ chính xác và hội tụ**| **Xấp xỉ, phương sai cao:** Kết quả MC là ước lượng dựa trên mẫu nên có **phương sai**. Cần rất nhiều mẫu để hội tụ về giá trị kỳ vọng thật ([Foundations of Reinforcement Learning  Model-free RL: Monte Carlo and temporal difference (TD) learning](https://users.ece.cmu.edu/~yuejiec/ece18813B_notes/lecture8-model-free-MC-TD.pdf#:~:text=Caveat%20of%20Monte%20Carlo%20methods%3A,11)). Khi số tập → ∞ thì $V(s)$ hội tụ chính xác (nếu mọi trạng thái được khám phá đủ). | **Chính xác (nếu có mô hình):** DP giải đúng hệ phương trình Bellman nên về lý thuyết có thể hội tụ đến giá trị *chính xác* của hàm giá trị (tất nhiên trong thực tế có thể dừng sau hữu hạn bước với sai số nhỏ). Không có sai số do mẫu, nhưng có thể có sai số do truncation nếu dừng sớm. |
| **Ưu điểm chính**       | - **Không cần mô hình**, có thể áp dụng trực tiếp lên môi trường thực hoặc mô phỏng. <br/>- **Dễ hiểu và cài đặt** (chỉ cần thu thập trung bình mẫu). <br/>- Có thể tập trung tính giá trị cho những phần quan trọng của không gian trạng thái (qua trải nghiệm) thay vì duyệt toàn bộ. | - **Tận dụng tri thức môi trường**: cho kết quả chính xác và thường hội tụ nhanh nếu không gian trạng thái nhỏ. <br/>- **Bảo đảm tối ưu toàn cục** khi có mô hình hoàn chỉnh (ví dụ Value Iteration luôn tìm được $\pi^*$ nếu tính toán đủ lâu). |
| **Nhược điểm chính**    | - **Đòi hỏi nhiều tập** để đạt kết quả ổn định; hiệu quả kém khi phần thưởng có độ biến động lớn (variance cao). <br/>- **Chỉ áp dụng tự nhiên cho episodic**; đối với continuing tasks cần thủ thuật (ví dụ cắt chuỗi) dễ gây sai số. <br/>- **Không có mô hình** nên không thể suy luận “giả lập” mà phải thử nghiệm thật, có thể tốn kém hoặc nguy hiểm trong môi trường thực. | - **Cần mô hình** – không áp dụng được nếu môi trường phức tạp, không biết xác suất hoặc không thể mô hình hóa. <br/>- **Bùng nổ tính toán** với bài toán lớn (state space, action space lớn) do phải duyệt qua mọi trạng thái, hoặc lưu trữ bảng giá trị khổng lồ. <br/>- Không sử dụng được trực tiếp trong môi trường thực không mô phỏng (vì không thể có mô hình xác suất chính xác). |

Ngoài DP, Monte Carlo cũng nên được so sánh với các phương pháp model-free khác, đặc biệt là **Temporal-Difference (TD) Learning**. TD (như SARSA, Q-learning) cũng không cần mô hình nhưng cập nhật giá trị *từng bước một* dựa trên giá trị ước lượng hiện tại (bootstrapping). So với TD, MC **không bootstrapping** nên không chịu bias do ước lượng tạm thời, nhưng TD có ưu thế **cập nhật liên tục mỗi bước** (không phải chờ hết tập) và thường **hội tụ nhanh hơn với ít phương sai hơn** do sử dụng thêm thông tin cấu trúc Markov. Trong thực tiễn, TD và các biến thể (như $n$-step TD, TD($\lambda$)) phổ biến hơn cho các tác vụ dài hạn hoặc khi dữ liệu thu thập online liên tục. Tuy nhiên, Monte Carlo vẫn là phương pháp nền tảng, trực quan và thường hoạt động tốt trong các môi trường mô phỏng với tập tương tác rõ ràng.

Tựu trung, Monte Carlo và Dynamic Programming đại diện cho hai thái cực: một bên dùng mẫu để ước lượng không cần mô hình, một bên dùng mô hình để tính toán chính xác. Lựa chọn phương pháp tùy thuộc vào hoàn cảnh: nếu có mô hình tin cậy và không gian trạng thái nhỏ, DP cho kết quả nhanh và chính xác; nếu mô hình phức tạp hoặc không biết, Monte Carlo là lựa chọn tự nhiên để “học từ trải nghiệm”. Nhiều thuật toán RL hiện đại kết hợp ý tưởng của cả hai (như TD kết hợp mẫu + bootstrapping, hay giả lập môi trường để áp dụng DP cục bộ trong thuật toán Monte Carlo Tree Search, v.v.).

## Điều kiện áp dụng và hạn chế của phương pháp Monte Carlo

Như đã đề cập, phương pháp Monte Carlo **thích hợp nhất cho các bài toán có tập tương tác ngắt quãng (episodic)** – mỗi tập phải có điểm dừng để tính toán được tổng thưởng. Điều kiện tiên quyết là mỗi tập *cuối cùng sẽ kết thúc* (dù sớm hay muộn) để ta thu được $G_t$. Với các bài toán continuing (chuỗi vô tận), người ta phải định nghĩa ngắt tập nhân tạo (ví dụ sau $N$ bước) hoặc sử dụng chiết khấu và coi việc cắt chuỗi ở độ dài đủ lớn như “kết thúc”. Tuy nhiên, cách này chỉ cho ước lượng xấp xỉ và có thể bias nếu cắt quá ngắn. Do đó, MC **không phù hợp lắm cho các nhiệm vụ không có hồi kết tự nhiên** (so với TD có thể xử lý tốt bằng cách cập nhật dần).

Monte Carlo yêu cầu **chính sách/thám hiểm phải bao phủ không gian trạng thái-hành động**: tức về dài hạn, mọi trạng thái và hành động (cần đánh giá) đều phải có *xác suất ghé thăm dương*. Nếu không, một số phần của môi trường sẽ không bao giờ được học giá trị. Điều này đòi hỏi hoặc sử dụng *exploring starts* hoặc duy trì $\epsilon$-greedy trong on-policy control, hoặc lựa chọn cẩn thận chính sách hành vi trong off-policy. Nói cách khác, **điều kiện khám phá đầy đủ (exploration)** là bắt buộc để đảm bảo tính đúng đắn của MC.

Về mặt hội tụ, MC dựa trên trung bình mẫu nên hiệu quả phụ thuộc nhiều vào **độ biến động (variance)** của phần thưởng. Nếu phần thưởng và quá trình chuyển trạng thái có nhiều ngẫu nhiên, returns thu được sẽ dao động mạnh, khiến việc hội tụ chậm. MC cho kết quả *không chệch* (unbiased) khi số mẫu tiến tới vô hạn, nhưng với số mẫu hữu hạn, phương sai cao có thể khiến ước lượng kém ổn định. Trường hợp extreme, như đã nêu với off-policy MC, **phương sai có thể tiến tới vô hạn** nếu dùng ordinary importance sampling không phù hợp ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=,and%20thus%20is%20strongly%20preferred)). Trong thực tế, người dùng MC thường phải chọn chiến lược giảm phương sai: ví dụ dùng weighted importance sampling, hoặc giảm dần $\epsilon$ trong $\epsilon$-greedy, hoặc đơn giản là tăng số lần chạy mô phỏng.

Một hạn chế khác: Monte Carlo đòi hỏi **nhiều thời gian và dữ liệu** nếu môi trường lớn. Mặc dù mỗi tập có thể chỉ khám phá một phần không gian trạng thái, để đảm bảo đủ độ phủ, ta vẫn cần rất nhiều tập. Nếu một số sự kiện/trạng thái hiếm khi xảy ra, MC sẽ mất rất lâu mới thu thập đủ dữ liệu để đánh giá chúng chính xác. Trong khi đó, nếu có mô hình, ta có thể tính toán giá trị các trạng thái hiếm thông qua suy luận thay vì chờ chúng xảy ra.

Cuối cùng, **tính thời gian thực**: MC thường được xem là phương pháp “offline” hoặc *batch learning*, vì ta thường chạy nhiều tập mô phỏng, sau đó mới tổng hợp kết quả để cập nhật giá trị/policy. Trong môi trường thực tế liên tục, việc chờ đến cuối “tập” (ví dụ kết thúc một trò chơi) mới điều chỉnh có thể không kịp thời. Các phương pháp bootstrapping (như TD) cập nhật liên tục mỗi bước nên phù hợp hơn cho learning online, còn MC phù hợp khi ta có thể thu thập dữ liệu thành các phiên rời rạc.

Tóm lại, Monte Carlo thích hợp khi:
- Môi trường **episodic rõ ràng** (có trạng thái kết thúc).
- **Không có mô hình** hoặc mô phỏng môi trường dễ hơn là phân tích mô hình.
- Muốn có ước lượng giá trị **không chệch** về dài hạn và chấp nhận việc cần nhiều mẫu.
- Bài toán cho phép thu thập một tập lớn dữ liệu trải nghiệm (ví dụ mô phỏng trò chơi, vận hành thử nghiệm nhiều lần).

Hạn chế của MC gồm:
- Không dùng tốt cho bài toán **tiếp diễn vô tận** (trừ khi thủ thuật chia tập).
- Cần **nhiều mẫu** để hội tụ – phương sai cao.
- **Chậm cập nhật** (chỉ cuối tập), khó dùng trong môi trường yêu cầu phản ứng nhanh.
- Đòi hỏi **chiến lược thăm dò** thích hợp; nếu không, kết quả học có thể sai lệch hoặc không đầy đủ.

Dù có hạn chế, Monte Carlo vẫn là công cụ mạnh trong RL, đặc biệt ở giai đoạn khởi đầu hoặc trong nghiên cứu lý thuyết, vì sự đơn giản và rõ ràng trong ý tưởng: “**Hãy chơi nhiều lần và học từ những gì đã xảy ra**”.

## Ví dụ minh họa chi tiết: Đánh giá chiến lược trong trò Blackjack bằng Monte Carlo

Để làm rõ hơn cách phương pháp Monte Carlo hoạt động, chúng ta xét **ví dụ trò chơi Blackjack** kinh điển trong tài liệu của Sutton & Barto ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=Example%3A%20Blackjack%20%E2%80%A2%20Goal%3A%20Achieve,20%20or%2021%2C%20else%20hit)). Blackjack (Xì dách) là trò chơi mà mục tiêu của người chơi là rút được tổng điểm các lá bài **gần 21 nhất nhưng không vượt quá 21**, đồng thời cao hơn điểm của nhà cái. Mỗi ván Blackjack là một *tập tương tác* tự nhiên: nó bắt đầu với việc chia bài và kết thúc khi người chơi dừng hoặc bị quá 21, sau đó so sánh điểm với nhà cái để xác định thắng/thua.

**Mô tả môi trường Blackjack đơn giản** (*theo mô hình trong sách* ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=Example%3A%20Blackjack%20%E2%80%A2%20Goal%3A%20Achieve,20%20or%2021%2C%20else%20hit))):

- **Trạng thái:** Được biểu diễn bởi bộ ba `(tổng điểm của người chơi, lá bài ngửa của nhà cái, có usable ace không)`. Ở đây *usable ace* nghĩa là người chơi có quân Át được tính là 11 điểm mà không bị quá 21. Tổng điểm của người chơi được giới hạn từ 12 đến 21 (vì dưới 12 điểm chắc chắn người chơi sẽ rút thêm), lá ngửa của dealer từ 1 (Át) đến 10 (10, J, Q, K đều tính 10 điểm). Như vậy không gian trạng thái có **200 trạng thái khả dĩ** ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=%E2%80%A2%20State%20space%3A%20,works%2011%2F10%2F2010%20Reinforcement%20Learning%2013)).
- **Hành động:** Gồm hai hành động chính: **“Hit” (rút thêm bài)** hoặc **“Stick” (dừng, không rút thêm)** ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=%E2%80%A2%20Reward%3A%20%2B1%20for%20win%2C,20%20or%2021%2C%20else%20hit)).
- **Phần thưởng:** +1 nếu người chơi thắng (điểm cao hơn dealer hoặc dealer quá 21), 0 nếu hòa, –1 nếu thua ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=%E2%80%93%20Dealer%E2%80%99s%20showing%20card%20,receive%20another%20card)). Phần thưởng chỉ nhận được khi kết thúc ván.
- **Chính sách của người chơi (cần đánh giá):** Ở ví dụ này, ta xét một chiến lược đơn giản: **rút bài cho đến khi đạt được 20 điểm hoặc 21 thì dừng**, cụ thể: *nếu tổng điểm hiện tại của người chơi $\ge 20$ thì Stick, ngược lại thì Hit* ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=%E2%80%A2%20Reward%3A%20%2B1%20for%20win%2C,works%2011%2F10%2F2010%20Reinforcement%20Learning%2013)). Đây là một chính sách cố định, không tối ưu nhưng hợp lý để minh họa (một dạng “chiến thuật cơ bản” giản lược).

Nhiệm vụ của ta: **ước lượng giá trị của mỗi trạng thái** dưới chính sách cố định trên bằng phương pháp Monte Carlo. Do không có mô hình xác suất chính xác cho Blackjack (bài trộn ngẫu nhiên), ta sẽ dùng mô phỏng Monte Carlo: cho người chơi và nhà cái chơi nhiều ván theo luật và theo chính sách đã cho của người chơi, rồi thống kê kết quả.

**Áp dụng Monte Carlo prediction:**

Ta thực hiện giả lập hàng trăm nghìn ván Blackjack. Trong mỗi ván, ta theo dõi chuỗi trạng thái và hành động của người chơi. Với chính sách trên, người chơi sẽ rút bài đến khi đạt 20 hoặc 21 điểm rồi dừng, sau đó phần thưởng được xác định (thắng/hòa/thua). Sử dụng *first-visit MC*, ta cập nhật giá trị cho **trạng thái đầu tiên** của mỗi ván (hoặc ta cũng có thể cập nhật cho mọi trạng thái trong ván – tuy nhiên do chính sách ở đây cố định và phần thưởng chỉ có khi kết thúc, giá trị các trạng thái trung gian có thể suy luận tương tự trạng thái đầu do Markov, nhưng chúng ta cứ làm tổng quát). Cụ thể: nếu trạng thái $s = (tổng, bài dealer, ace)$ xuất hiện lần đầu trong ván và kết quả ván là thắng (+1), ta cộng +1 vào danh sách returns của $s$. Nếu thua, cộng -1, hòa thì 0. Sau rất nhiều ván, giá trị ước lượng $V(s)$ cho trạng thái $s$ sẽ là trung bình các kết quả thu được từ $s$. 

Monte Carlo không những cho ta ước lượng giá trị của từng trạng thái mà còn có thể cho cái nhìn trực quan về chiến lược. **Hình dưới đây** minh họa *hàm giá trị trạng thái $V^{\pi}(s)$ ước lượng được cho chính sách trên trong Blackjack*, phân biệt hai trường hợp: khi người chơi **có** một Át hữu dụng (trái) và **không có** Át hữu dụng (phải). Biểu đồ bên trái của mỗi cặp là giá trị ước lượng sau 10.000 ván, bên phải là sau 500.000 ván (gần hội tụ) ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent#:~:text=The%20left%20hand%20graphs%20are,will%20look%20like%20before%20convergence)):

 ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent)) *Hàm giá trị ước lượng của chính sách "rút đến 20 rồi dừng" trong Blackjack, phân tách theo trường hợp người chơi có **usable ace** (trên) hoặc không có (dưới). Mỗi đồ thị bên trái ứng với sau 10.000 ván (chưa hội tụ), bên phải ứng với sau 500.000 ván (gần hội tụ). Trục hoành là lá bài ngửa của dealer (A=Át, 2-10), trục tung là tổng điểm của người chơi (12-21). Giá trị trạng thái được biểu diễn theo thang màu độ cao, từ -1 (thua chắc) đến +1 (thắng chắc).* ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent#:~:text=The%20left%20hand%20graphs%20are,will%20look%20like%20before%20convergence)) ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent#:~:text=,build%20the%20chart%20is%20lower))

Nhìn vào kết quả, ta thấy sau khoảng **500.000 ván mô phỏng**, hàm giá trị đã khá ổn định (bề mặt giá trị trơn tru hơn), phản ánh chính xác chất lượng các trạng thái dưới chiến lược đã cho. Những trạng thái “ tốt ” – ví dụ tổng điểm cao (gần 21) – có giá trị dương (cơ hội thắng cao), trong khi những trạng thái “xấu” – điểm thấp, đặc biệt khi dealer có lá lớn – có giá trị âm (dễ thua). Sự khác biệt giữa trường hợp **có Át hữu dụng** và **không có Át** cũng được Monte Carlo thể hiện: Với một Át mềm (usable ace), người chơi có lợi thế linh hoạt (Át có thể tính 11 hoặc 1), do đó ngay cả với tổng điểm chưa cao, xác suất thắng có thể nhỉnh hơn so với khi không có Át. Chẳng hạn, ở tổng điểm ~15 và dealer đang có 4, nếu người chơi có Át mềm thì giá trị kỳ vọng có thể dương (có cơ hội thắng) do khả năng điều chỉnh Át, trong khi không có Át thì tình huống đó khá bất lợi. Điều này giải thích tại sao bề mặt giá trị “có Át” nhìn chung cao hơn một chút và cũng **hội tụ chậm hơn** – các trạng thái với Át hiếm gặp hơn (xác suất ~15% mỗi ván) và kết quả biến động hơn do sự linh hoạt của Át ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent#:~:text=,build%20the%20chart%20is%20lower)), nên cần nhiều ván hơn để ước lượng chính xác.

Monte Carlo đã thành công trong việc **dự đoán giá trị** cho từng trạng thái Blackjack theo chính sách đã cho một cách gần đúng. Từ kết quả này, ta có thể **cải thiện chính sách**: ví dụ, nếu ở một số trạng thái cụ thể giá trị của hành động “Hit” cao hơn “Stick” (mà chính sách ban đầu chưa chọn tối ưu), ta có thể điều chỉnh chiến lược. Thật vậy, Sutton & Barto tiếp tục dùng phương pháp **Monte Carlo control** (với exploring starts) trên trò Blackjack và thu được **chính sách tối ưu** gần giống chiến lược chơi tối ưu mà các cao thủ blackjack khuyến nghị. Điều này minh họa sức mạnh của MC: chỉ bằng cách chơi thử và ghi nhận kết quả, phương pháp đã tự động tìm ra chiến lược tốt.

**Kết luận ví dụ:** Trong trò Blackjack, Monte Carlo cung cấp một cách tiếp cận tự nhiên: chơi nhiều ván và **mô phỏng trải nghiệm**. Thông qua việc ghi nhận thắng/thua và trung bình cộng, ta đánh giá được chất lượng của các trạng thái. So sánh với cách phân tích toán xác suất của trò chơi (vốn rất phức tạp do nhiều tình huống bài), phương pháp MC đơn giản hơn nhiều – ta không cần giải công thức nào, chỉ cần đủ dữ liệu. Ví dụ này cho thấy Monte Carlo rất hữu dụng trong những tình huống mà mô phỏng dễ dàng nhưng phân tích lý thuyết thì khó. Chính nhờ khả năng “thực nghiệm” này, phương pháp Monte Carlo trở thành một trong những công cụ quan trọng trong học tăng cường, đặt nền móng cho các thuật toán hiện đại hơn trong lĩnh vực trí tuệ nhân tạo. 

**Nguồn tài liệu tham khảo:** Sutton & Barto (2018) – *Reinforcement Learning: An Introduction* ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=First%20learning%20method%20for%20estimating,we%20learn%20only%20by%20experience)) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=An%20obvious%20way%20to%20estimate,converge%20to%20the%20expected%20value)) ([Sutton & Barto summary chap 05 - Monte Carlo methods | lcalem](https://lcalem.github.io/blog/2018/10/22/sutton-chap05-montecarlo#:~:text=We%20can%20apply%20backup%20diagrams,instead%20of%20all%20possible%20transitions)); Shiyu Zhao (2025) – *Mathematical Foundations of Reinforcement Learning* ([](https://page-one.springer.com/pdf/preview/10.1007/978-981-97-3944-8_5#:~:text=based%20on%20the%20system%20model,If%20we%20do%20not%20have)); cùng các ghi chú, bài giảng RL kinh điển ([Foundations of Reinforcement Learning  Model-free RL: Monte Carlo and temporal difference (TD) learning](https://users.ece.cmu.edu/~yuejiec/ece18813B_notes/lecture8-model-free-MC-TD.pdf#:~:text=Dynamic%20programming%20versus%20Monte%20Carlo,11)) ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=Example%3A%20Blackjack%20%E2%80%A2%20Goal%3A%20Achieve,20%20or%2021%2C%20else%20hit)). Các ví dụ và phân tích cụ thể về Blackjack được trích từ Sutton & Barto (Ch.5) ([Structure and Synthesis of Robot Motion  Introduction ](https://www.inf.ed.ac.uk/teaching/courses/rl/slides10/5_MC1.pdf#:~:text=Example%3A%20Blackjack%20%E2%80%A2%20Goal%3A%20Achieve,20%20or%2021%2C%20else%20hit)) và luận giải bởi cộng đồng AI ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent#:~:text=The%20left%20hand%20graphs%20are,will%20look%20like%20before%20convergence)) ([reinforcement learning - What does the figure "Blackjack Value Function..." from Sutton represent? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/17808/what-does-the-figure-blackjack-value-function-from-sutton-represent#:~:text=,build%20the%20chart%20is%20lower)).